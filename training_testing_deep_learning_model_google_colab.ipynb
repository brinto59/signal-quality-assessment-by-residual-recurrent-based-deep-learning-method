{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMjZpsdxKPla3xZ86588nwr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"eZgNrT7_3Esq","executionInfo":{"status":"ok","timestamp":1697062868473,"user_tz":-360,"elapsed":8354,"user":{"displayName":"Basob Paul Brinta","userId":"06538266531258667071"}}},"outputs":[],"source":["import torch\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchsummary\n","import scipy.io as sio\n","from torch.utils.data import DataLoader,Dataset,TensorDataset\n","from torch.autograd import Variable\n","import numpy as np\n","import h5py\n","from sklearn.preprocessing import minmax_scale\n","import torch.optim as optim\n","from keras.utils import to_categorical\n","from sklearn.model_selection import StratifiedKFold\n","from scipy.fftpack import fft, ifft\n","import scipy.signal as signal\n","def zscore(data):\n","    data_mean=np.mean(data)\n","    data_std=np.std(data, axis=0)\n","    if data_std!=0:\n","        data=(data-data_mean)/data_std\n","    else:\n","        data=data-data_mean\n","    return data\n","def butthigh(ecg, fs):\n","    b1 = np.array([0.995155038209359, -1.99031007641872, 0.995155038209359])\n","    a1 = np.array([1, -1.99028660262621, 0.990333550211225])\n","    ecg_copy = np.copy(ecg)\n","    ecg1 = signal.filtfilt(b1, a1, ecg_copy)\n","    return ecg1\n","def hobalka(ecg1, fs, fmin, fmax):\n","    ecg = np.copy(ecg1)\n","    n = len(ecg)\n","    ecg_fil = fft(ecg)\n","    if fmin > 0:\n","        imin = int(fmin / (fs / n))\n","    else:\n","        imin = 1\n","        ecg_fil[0] = ecg_fil[0] / 2\n","    if fmax < fs / 2:\n","        imax = int(fmax / float(fs / n))\n","    else:\n","        imax = int(n / 2)\n","    hamwindow = np.hamming(imax - imin)\n","    hamsize = len(hamwindow)\n","    yy = np.zeros(len(ecg_fil), dtype=complex)\n","    istred = int((imax + imin) / 2)\n","    dolni = np.arange(istred-1, imax)\n","    ld = len(dolni)\n","    yy[0: ld] = np.multiply(ecg_fil[dolni - 1], hamwindow[int(np.floor(hamsize / 2)) - 1: hamsize])\n","    horni = np.arange(imin-1, istred-1)\n","    lh = len(horni)\n","    end = len(yy)\n","    yy[end - lh - 1: end - 1] = np.multiply(ecg_fil[horni], hamwindow[0: int(np.floor(hamsize / 2))])\n","    ecg_fil = abs(ifft(yy)) * 2\n","    return ecg_fil\n","\n","class  conv1d_inception_block(nn.Module):\n","\n","    def __init__(self, in_ch, out_ch):\n","        super(conv1d_inception_block, self).__init__()\n","\n","        self.conv1_1 = nn.Sequential(\n","            nn.Conv1d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n","            nn.Dropout(0.3))\n","        self.conv1_3 = nn.Sequential(\n","            nn.Conv1d(in_ch, out_ch, kernel_size=5, stride=1, padding=2, bias=True),\n","            nn.Dropout(0.3))\n","        self.conv1_5 = nn.Sequential(\n","            nn.Conv1d(in_ch, out_ch, kernel_size=7, stride=1, padding=3, bias=True),\n","            nn.Dropout(0.3))\n","        self.conv= nn.Sequential(\n","            nn.Conv1d(in_ch, out_ch, kernel_size=1, stride=1, padding=0, bias=True),\n","            #nn.Conv1d(in_ch, out_ch, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.Dropout(0.3),\n","            nn.BatchNorm1d(out_ch),\n","            nn.ReLU())\n","    def forward(self, x):\n","\n","        x1 = self.conv1_1(x)\n","        x3 = self.conv1_3(x)\n","        x5 = self.conv1_5(x)\n","        return self.conv(x1+x3+x5)\n","class Recurrent_block(nn.Module):\n","\n","    def __init__(self, out_ch, t=2):\n","        super(Recurrent_block, self).__init__()\n","        #self.drop_layer = nn.Dropout(0.5)\n","        self.t = t\n","        self.out_ch = out_ch\n","        self.conv = nn.Sequential(\n","\n","            conv1d_inception_block(out_ch,out_ch),\n","            nn.Dropout(0.3),\n","            nn.BatchNorm1d(out_ch),\n","            nn.ReLU()\n","        )\n","        self.conv1_1 = nn.Sequential(\n","            nn.Conv1d(out_ch, out_ch, kernel_size=1, stride=1, padding=0, bias=True))\n","    def forward(self, x):\n","        for i in range(self.t):\n","            if i == 0:\n","                x1 = self.conv(x)\n","            out = self.conv1_1(x1 + x)   # out variable ta x1 howar kotha chilo na??\n","        return out\n","\n","class Residual_block(nn. Module):\n","\n","    def __init__(self, out_ch, t=2):\n","        super(Residual_block, self).__init__()\n","        #self.drop_layer = nn.Dropout(0.5)\n","        self.t = t\n","        self.out_ch = out_ch\n","        self.conv = nn.Sequential(\n","\n","            conv1d_inception_block(out_ch,out_ch),\n","            nn.Dropout(0.3),\n","            nn.BatchNorm1d(out_ch),\n","            nn.ReLU()\n","        )\n","        self.conv1_1 = nn.Sequential(\n","            nn.Conv1d(out_ch, out_ch, kernel_size=1, stride=1, padding=0, bias=True))\n","    def forward(self, x):\n","        x1 = self.conv(x)\n","        x1 = self.conv(x1)\n","        out=self.conv1_1(x1+x)\n","        return out\n","\n","\n","\n","\n","\n","\n","class R_1Dcnn_RCNN_block(nn.Module):\n","    def __init__(self, in_ch, out_ch, t=2):\n","        super(R_1Dcnn_RCNN_block, self).__init__()\n","\n","        self.RCNN1 = nn.Sequential(\n","            Recurrent_block(out_ch, t=t))\n","\n","        self.RCNN2 = nn.Sequential(\n","            conv1d_inception_block(out_ch, out_ch))\n","\n","        self.RCNN3 =nn.Sequential(\n","            Residual_block(out_ch, out_ch))\n","\n","        self.Conv = nn.Conv1d(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n","        self.Conv1_1 = nn.Sequential(\n","            nn.Conv1d(out_ch, out_ch, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm1d(out_ch),\n","            nn.ReLU())\n","    def forward(self, x):\n","        x=self.Conv(x)\n","        x1 = self.RCNN3(x)\n","        x2 = self.RCNN2(x)\n","        x3 = self.RCNN1(x)\n","        out = self.Conv1_1(x3+x2+x1)\n","        return out\n","'''\n","class Attention_block_self(nn.Module):\n","\n","    def __init__(self, F_l, F_int):\n","        super(Attention_block_self, self).__init__()\n","\n","        self.W_g = nn.Sequential(\n","            nn.Conv1d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm1d(F_int)\n","        )\n","\n","        self.psi = nn.Sequential(\n","            nn.Conv1d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm1d(1),\n","            nn.Sigmoid()\n","        )\n","\n","        self.Tanh = nn.Tanh()\n","\n","    def forward(self,  x):\n","        x1 = self.W_g(x)\n","        psi = self.Tanh(x1)\n","        psi = self.psi(psi)\n","        out = x * psi\n","        return out\n","'''\n","\n","class model_1d(nn.Module):\n","\n","    def __init__(self, img_ch=1, output_ch=1, t=1):\n","        super(model_1d, self).__init__()\n","\n","        n1 =6\n","        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n","        self.Maxpool0 = nn.MaxPool1d(kernel_size=2, stride=2)\n","        self.Maxpool1 = nn.MaxPool1d(kernel_size=8, stride=8)\n","        self.Maxpool2 = nn.MaxPool1d(kernel_size=4, stride=4)\n","        self.RRCNN1 = R_1Dcnn_RCNN_block(img_ch, filters[3], t=t)\n","        self.RRCNN2 = R_1Dcnn_RCNN_block(filters[3], filters[2], t=t)\n","        self.RRCNN3 = R_1Dcnn_RCNN_block(filters[2], filters[2], t=t)\n","        self.Softmax = nn.LogSoftmax()\n","        self.fc1 = nn.Linear(168 ,2)\n","    def forward(self, x):\n","        x=self.Maxpool0(x)\n","        e1 = self.RRCNN1(x)\n","        # print(e1.size())\n","        e2 = self.Maxpool2(e1)\n","        # print(e2.size())\n","        e2 = self.RRCNN2(e2)\n","        e3 = self.Maxpool2(e2)\n","        e3 = self.RRCNN3(e3)\n","        e3 = self.Maxpool2(e3)\n","        # print(e3.size())\n","        e4=self.Maxpool2(e3)\n","        # print(e4.size())\n","        e7= e4.view(e4.size(0),e4.size(1)*e4.size(2))\n","        # print(e7.size())\n","        out = self.fc1(e7)\n","        out =self.Softmax(out)\n","        return out\n","\n","\n","def train(x,model,los_train, acc_train):\n","    optimizer = torch.optim.SGD(model.parameters(),lr=0.0050,  momentum=0.90, dampening=0, weight_decay=0.0001, nesterov=False)\n","    epoch=x\n","    j=x\n","    model.train()\n","    running_loss = 0.0\n","    train_correct=0.0\n","    for i,data in enumerate(trainloader):\n","        inputs, labels = data\n","        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n","        optimizer.zero_grad()  # zero the gradient buffers\n","        output =  model(inputs)\n","        accracy =np.mean( (torch.argmax(output.cpu(),1)==torch.argmax(labels.cpu(),1)).numpy())\n","        # accracy =np.mean( (torch.argmax(output.cuda(),1)==torch.argmax(labels.cuda(),1)).numpy())\n","        loss = criterion(output, torch.argmax(labels, dim=1))\n","        loss.backward()\n","        optimizer.step()\n","        running_loss+=loss.item()\n","        train_correct+=accracy\n","        if i % np.ceil(len(trainloader.dataset)/32) == np.ceil(len(trainloader.dataset)/32)-1:    # print every 2000 mini-batches\n","            print('[%d, %5d] loss: %.3f  train_acc:%.4f'%(epoch + 1, i + 1, running_loss /(len(trainloader.dataset)/32), train_correct/(len(trainloader.dataset)/32)))\n","            los_train[j]= running_loss /(len(trainloader.dataset)/32)\n","            acc_train[j]= train_correct/(len(trainloader.dataset)/32)\n","            running_loss = 0.0\n","            train_correct=0.0\n","    return los_train, acc_train\n","\n","def validation(j,model,los_test, acc_test):\n","     model.eval()\n","     test_loss = 0.0\n","     test_correct=0.0\n","     for inputs1, labels1 in testloader:\n","        inputs1, labels1 = Variable(inputs1.cuda()), Variable(labels1.cuda())\n","        output =  model(inputs1)\n","        accracy1 =(torch.argmax(output.cpu(),1)==torch.argmax(labels1.cpu(),1)).numpy().sum()\n","        loss1 = criterion(output, torch.argmax(labels1, dim=1))\n","        test_loss+=loss1.item()\n","        test_correct+=accracy1\n","     test_loss /= (len(testloader.dataset)/32)\n","     test_correct/=(len(testloader.dataset))\n","     los_test[j]= test_loss\n","     acc_test[j]= test_correct\n","     print('test_loss:%.4f, test_correct%.4f'%(test_loss,test_correct))\n","\n","def test():\n","     model.eval()\n","     test_loss = 0.0\n","     test_correct=0.0\n","     for inputs1, labels1 in testloader:\n","        inputs1, labels1 = Variable(inputs1.cuda()), Variable(labels1.cuda())\n","        output =  model(inputs1)\n","        accracy1 =(torch.argmax(output.cpu(),1)==torch.argmax(labels1.cpu(),1)).numpy().sum()\n","        loss1 = criterion(output, torch.argmax(labels1, dim=1))\n","        test_loss+=loss1.item()\n","        test_correct+=accracy1\n","     test_loss /= (len(testloader.dataset)/32)  # 32 is batch size\n","     test_correct/=(len(testloader.dataset))\n","     print('test_loss:%.4f, test_correct%.4f'%(test_loss,test_correct))\n"]},{"cell_type":"code","source":["# import pickle\n","# with open('training_dataset_2.pkl', 'rb') as f:\n","#      dataset = pickle.load(f)"],"metadata":{"id":"Noad2Spm3hQQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = sio.loadmat('ecgpart_02.mat')['ecgpart']\n","data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kC3It6b5LCum","executionInfo":{"status":"ok","timestamp":1697062883944,"user_tz":-360,"elapsed":789,"user":{"displayName":"Basob Paul Brinta","userId":"06538266531258667071"}},"outputId":"8b81743d-1d52-4dfd-8e03-7fd0f1428219"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8218, 4000)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["label = sio.loadmat('CSPC2020label_02_0_1_by_manurally.mat')['sqi']\n","label = label.reshape(8218, 1)\n","label.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N_2UOWzu4Ho5","executionInfo":{"status":"ok","timestamp":1697062912318,"user_tz":-360,"elapsed":558,"user":{"displayName":"Basob Paul Brinta","userId":"06538266531258667071"}},"outputId":"06293919-3bb1-4ede-c5f9-a62272a670f0"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8218, 1)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["from sklearn.model_selection import StratifiedShuffleSplit  # spliting into training and testing dataset\n","sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n","(train_index, test_index) = sss.split(data, label).__next__()"],"metadata":{"id":"FPS4_AwjNFkU","executionInfo":{"status":"ok","timestamp":1697062917761,"user_tz":-360,"elapsed":620,"user":{"displayName":"Basob Paul Brinta","userId":"06538266531258667071"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","main_dataset = pd.DataFrame(np.concatenate((data, label), axis=1))\n","dataset = main_dataset.iloc[train_index].values\n"],"metadata":{"id":"cheV1vrOQPIM","executionInfo":{"status":"ok","timestamp":1697062921496,"user_tz":-360,"elapsed":568,"user":{"displayName":"Basob Paul Brinta","userId":"06538266531258667071"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["dataset.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjDItGrJP89f","executionInfo":{"status":"ok","timestamp":1697030335559,"user_tz":-360,"elapsed":540,"user":{"displayName":"Basob Paul Brinta","userId":"06538266531258667071"}},"outputId":"e209eb60-91d2-4d27-8a38-7d53698d9f63"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6574, 4001)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["ecga = dataset[:, 0:4000]\n","labela = dataset[:, -1].reshape(6574, 1)\n","\n","file_number=0\n","sfolder = StratifiedKFold(n_splits=10,random_state=1,shuffle=True)\n","for traindata, testdata in sfolder.split(ecga,labela):\n","    file_number=file_number+1\n","    ecgc=ecga[traindata]\n","    ecgt=ecga[testdata]\n","    labelc=labela[traindata]\n","    labelt=labela[testdata]\n","    for FF in range(len(ecgc)):\n","        ecgc[FF,:]=butthigh(zscore(ecgc[FF,:]),4000)\n","    for FF1 in range(len(ecgt)):\n","        ecgt[FF1,:]=butthigh(zscore(ecgt[FF1,:]),4000)\n","\n","    ecgc=torch.FloatTensor(ecgc)\n","    ecgc=ecgc.unsqueeze(1)\n","    labelc=to_categorical(labelc)\n","    labelc=torch.FloatTensor(labelc)\n","    deal_dataset = TensorDataset(ecgc,labelc)\n","    trainloader=DataLoader(dataset=deal_dataset,batch_size=32,shuffle=True,num_workers=0)\n","\n","    ecgt=torch.FloatTensor(ecgt)\n","    ecgt=ecgt.unsqueeze(1)\n","    labelt=to_categorical(labelt)\n","    labelt=torch.FloatTensor(labelt)\n","    deal_test_dataset = TensorDataset(ecgt,labelt)\n","    testloader=DataLoader(dataset=deal_test_dataset,batch_size=32,shuffle=True,num_workers=0)\n","\n","    model=model_1d()\n","    model = model.cuda()\n","    #torchsummary.summary(model.cuda(), input_size=(1,512,241))\n","    criterion = nn.CrossEntropyLoss()\n","    #optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.8)\n","    los_train=np.zeros(200)\n","    acc_train=np.zeros(200)\n","    los_test=np.zeros(200)\n","    acc_test=np.zeros(200)\n","    for x in range(200):\n","        los_train, acc_train=train(x,model,los_train, acc_train)\n","        validation(x,model,los_test, acc_test)\n","        if acc_test[x]>0.85:\n","            matname='conmodel_'+str(file_number)+'_epoch'+str(x)+'.pkl'\n","            torch.save(model, matname)\n","    filename_save='c11inception_10foldaccloss_model'+str(file_number)+'.mat'\n","    sio.savemat(filename_save, {'los_train':los_train,'acc_train':acc_train,'los_test':los_test,'acc_test':acc_test})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"QVpOcfAN4Izl","outputId":"5a02a05e-4e81-4a8f-eb56-8b07853c91ff","executionInfo":{"status":"error","timestamp":1697039079854,"user_tz":-360,"elapsed":8670413,"user":{"displayName":"Basob Paul Brinta","userId":"06538266531258667071"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-1-7dd1ea3725cf>:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  out =self.Softmax(out)\n"]},{"output_type":"stream","name":"stdout","text":["[1,   185] loss: 0.289  train_acc:0.8693\n","test_loss:0.2734, test_correct0.9012\n","[2,   185] loss: 0.136  train_acc:0.9437\n","test_loss:0.3489, test_correct0.8617\n","[3,   185] loss: 0.104  train_acc:0.9586\n","test_loss:0.1482, test_correct0.9605\n","[4,   185] loss: 0.100  train_acc:0.9635\n","test_loss:0.1721, test_correct0.9574\n","[5,   185] loss: 0.094  train_acc:0.9663\n","test_loss:0.1551, test_correct0.9650\n","[6,   185] loss: 0.092  train_acc:0.9668\n","test_loss:0.0973, test_correct0.9726\n","[7,   185] loss: 0.089  train_acc:0.9643\n","test_loss:0.1423, test_correct0.9650\n","[8,   185] loss: 0.089  train_acc:0.9697\n","test_loss:0.0914, test_correct0.9726\n","[9,   185] loss: 0.079  train_acc:0.9732\n","test_loss:0.0970, test_correct0.9711\n","[10,   185] loss: 0.082  train_acc:0.9701\n","test_loss:0.1115, test_correct0.9726\n","[11,   185] loss: 0.081  train_acc:0.9716\n","test_loss:0.0923, test_correct0.9726\n","[12,   185] loss: 0.075  train_acc:0.9738\n","test_loss:0.1466, test_correct0.9666\n","[13,   185] loss: 0.075  train_acc:0.9733\n","test_loss:0.1334, test_correct0.9696\n","[14,   185] loss: 0.076  train_acc:0.9731\n","test_loss:0.1186, test_correct0.9757\n","[15,   185] loss: 0.080  train_acc:0.9718\n","test_loss:0.1308, test_correct0.9787\n","[16,   185] loss: 0.073  train_acc:0.9743\n","test_loss:0.0944, test_correct0.9726\n","test_loss:0.1053, test_correct0.9726\n","[18,   185] loss: 0.070  train_acc:0.9746\n","test_loss:0.1022, test_correct0.9742\n","[19,   185] loss: 0.072  train_acc:0.9753\n","test_loss:0.0960, test_correct0.9757\n","[20,   185] loss: 0.072  train_acc:0.9745\n","test_loss:0.1291, test_correct0.9711\n","[21,   185] loss: 0.068  train_acc:0.9768\n","test_loss:0.0973, test_correct0.9757\n","[22,   185] loss: 0.070  train_acc:0.9760\n","test_loss:0.1904, test_correct0.9681\n","[23,   185] loss: 0.068  train_acc:0.9750\n","test_loss:0.1133, test_correct0.9726\n","[24,   185] loss: 0.064  train_acc:0.9785\n","test_loss:0.0991, test_correct0.9757\n","[25,   185] loss: 0.066  train_acc:0.9765\n","test_loss:0.1066, test_correct0.9772\n","[26,   185] loss: 0.065  train_acc:0.9795\n","test_loss:0.1138, test_correct0.9772\n","[27,   185] loss: 0.062  train_acc:0.9774\n","test_loss:0.1099, test_correct0.9726\n","[28,   185] loss: 0.070  train_acc:0.9750\n","test_loss:0.0942, test_correct0.9742\n","[29,   185] loss: 0.062  train_acc:0.9799\n","test_loss:0.1297, test_correct0.9772\n","[30,   185] loss: 0.064  train_acc:0.9784\n","test_loss:0.1150, test_correct0.9757\n","[31,   185] loss: 0.062  train_acc:0.9794\n","test_loss:0.1208, test_correct0.9711\n","[32,   185] loss: 0.064  train_acc:0.9775\n","test_loss:0.0931, test_correct0.9726\n","[33,   185] loss: 0.063  train_acc:0.9768\n","test_loss:0.1042, test_correct0.9787\n","[34,   185] loss: 0.059  train_acc:0.9819\n","test_loss:0.1144, test_correct0.9772\n","[35,   185] loss: 0.062  train_acc:0.9780\n","test_loss:0.1017, test_correct0.9726\n","[36,   185] loss: 0.060  train_acc:0.9797\n","test_loss:0.1102, test_correct0.9757\n","[37,   185] loss: 0.059  train_acc:0.9806\n","test_loss:0.1026, test_correct0.9772\n","[38,   185] loss: 0.057  train_acc:0.9804\n","test_loss:0.1119, test_correct0.9772\n","[39,   185] loss: 0.063  train_acc:0.9773\n","test_loss:0.1074, test_correct0.9772\n","[40,   185] loss: 0.063  train_acc:0.9775\n","test_loss:0.1065, test_correct0.9787\n","[41,   185] loss: 0.063  train_acc:0.9787\n","test_loss:0.0976, test_correct0.9742\n","[42,   185] loss: 0.059  train_acc:0.9812\n","test_loss:0.0950, test_correct0.9802\n","[43,   185] loss: 0.060  train_acc:0.9801\n","test_loss:0.1178, test_correct0.9772\n","[44,   185] loss: 0.059  train_acc:0.9795\n","test_loss:0.1033, test_correct0.9787\n","[45,   185] loss: 0.057  train_acc:0.9817\n","test_loss:0.0971, test_correct0.9772\n","[46,   185] loss: 0.059  train_acc:0.9799\n","test_loss:0.0981, test_correct0.9787\n","[47,   185] loss: 0.057  train_acc:0.9821\n","test_loss:0.1074, test_correct0.9726\n","[48,   185] loss: 0.055  train_acc:0.9819\n","test_loss:0.1030, test_correct0.9726\n","[49,   185] loss: 0.061  train_acc:0.9797\n","test_loss:0.1058, test_correct0.9772\n","[50,   185] loss: 0.058  train_acc:0.9804\n","test_loss:0.1144, test_correct0.9787\n","[51,   185] loss: 0.056  train_acc:0.9807\n","test_loss:0.1097, test_correct0.9757\n","[52,   185] loss: 0.057  train_acc:0.9799\n","test_loss:0.1101, test_correct0.9787\n","[53,   185] loss: 0.056  train_acc:0.9819\n","test_loss:0.1163, test_correct0.9742\n","[54,   185] loss: 0.053  train_acc:0.9809\n","test_loss:0.1048, test_correct0.9757\n","[55,   185] loss: 0.060  train_acc:0.9782\n","test_loss:0.1065, test_correct0.9772\n","[56,   185] loss: 0.053  train_acc:0.9820\n","test_loss:0.1170, test_correct0.9757\n","[57,   185] loss: 0.056  train_acc:0.9799\n","test_loss:0.1239, test_correct0.9742\n","[58,   185] loss: 0.055  train_acc:0.9824\n","test_loss:0.0928, test_correct0.9787\n","[59,   185] loss: 0.052  train_acc:0.9833\n","test_loss:0.0931, test_correct0.9787\n","[60,   185] loss: 0.050  train_acc:0.9844\n","test_loss:0.0963, test_correct0.9726\n","[61,   185] loss: 0.053  train_acc:0.9816\n","test_loss:0.1036, test_correct0.9742\n","[62,   185] loss: 0.057  train_acc:0.9799\n","test_loss:0.0917, test_correct0.9757\n","[63,   185] loss: 0.054  train_acc:0.9799\n","test_loss:0.1000, test_correct0.9772\n","[64,   185] loss: 0.054  train_acc:0.9801\n","test_loss:0.0898, test_correct0.9772\n","[65,   185] loss: 0.049  train_acc:0.9846\n","test_loss:0.0816, test_correct0.9818\n","[66,   185] loss: 0.051  train_acc:0.9831\n","test_loss:0.1180, test_correct0.9757\n","[67,   185] loss: 0.053  train_acc:0.9836\n","test_loss:0.0994, test_correct0.9772\n","[68,   185] loss: 0.056  train_acc:0.9787\n","test_loss:0.1078, test_correct0.9711\n","[69,   185] loss: 0.048  train_acc:0.9843\n","test_loss:0.1085, test_correct0.9787\n","[70,   185] loss: 0.052  train_acc:0.9829\n","test_loss:0.0808, test_correct0.9802\n","[71,   185] loss: 0.050  train_acc:0.9850\n","test_loss:0.0950, test_correct0.9757\n","[72,   185] loss: 0.050  train_acc:0.9829\n","test_loss:0.0797, test_correct0.9772\n","[73,   185] loss: 0.051  train_acc:0.9826\n","test_loss:0.0810, test_correct0.9772\n","[74,   185] loss: 0.051  train_acc:0.9824\n","test_loss:0.0933, test_correct0.9787\n","[75,   185] loss: 0.048  train_acc:0.9833\n","test_loss:0.0977, test_correct0.9757\n","[76,   185] loss: 0.056  train_acc:0.9809\n","test_loss:0.0951, test_correct0.9757\n","[77,   185] loss: 0.049  train_acc:0.9821\n","test_loss:0.0973, test_correct0.9787\n","[78,   185] loss: 0.049  train_acc:0.9838\n","test_loss:0.1093, test_correct0.9772\n","[79,   185] loss: 0.048  train_acc:0.9829\n","test_loss:0.0966, test_correct0.9787\n","[80,   185] loss: 0.048  train_acc:0.9830\n","test_loss:0.0827, test_correct0.9772\n","[81,   185] loss: 0.050  train_acc:0.9826\n","test_loss:0.0812, test_correct0.9787\n","[82,   185] loss: 0.049  train_acc:0.9844\n","test_loss:0.0930, test_correct0.9757\n","[83,   185] loss: 0.047  train_acc:0.9834\n","test_loss:0.0869, test_correct0.9757\n","[84,   185] loss: 0.049  train_acc:0.9823\n","test_loss:0.0888, test_correct0.9787\n","[85,   185] loss: 0.051  train_acc:0.9841\n","test_loss:0.0899, test_correct0.9818\n","[86,   185] loss: 0.050  train_acc:0.9839\n","test_loss:0.0946, test_correct0.9787\n","[87,   185] loss: 0.048  train_acc:0.9833\n","test_loss:0.0924, test_correct0.9802\n","[88,   185] loss: 0.046  train_acc:0.9844\n","test_loss:0.0942, test_correct0.9757\n","[89,   185] loss: 0.045  train_acc:0.9839\n","test_loss:0.0911, test_correct0.9787\n","[90,   185] loss: 0.046  train_acc:0.9839\n","test_loss:0.0816, test_correct0.9787\n","[91,   185] loss: 0.049  train_acc:0.9838\n","test_loss:0.0812, test_correct0.9772\n","[92,   185] loss: 0.051  train_acc:0.9806\n","test_loss:0.0906, test_correct0.9772\n","[93,   185] loss: 0.045  train_acc:0.9851\n","test_loss:0.0978, test_correct0.9772\n","[94,   185] loss: 0.045  train_acc:0.9858\n","test_loss:0.0859, test_correct0.9742\n","[95,   185] loss: 0.046  train_acc:0.9858\n","test_loss:0.0824, test_correct0.9772\n","[96,   185] loss: 0.044  train_acc:0.9866\n","test_loss:0.1014, test_correct0.9757\n","[97,   185] loss: 0.043  train_acc:0.9863\n","test_loss:0.1014, test_correct0.9711\n","[98,   185] loss: 0.045  train_acc:0.9844\n","test_loss:0.0994, test_correct0.9757\n","[99,   185] loss: 0.042  train_acc:0.9861\n","test_loss:0.0941, test_correct0.9772\n","[100,   185] loss: 0.044  train_acc:0.9838\n","test_loss:0.0833, test_correct0.9772\n","[101,   185] loss: 0.043  train_acc:0.9848\n","test_loss:0.0991, test_correct0.9802\n","[102,   185] loss: 0.044  train_acc:0.9844\n","test_loss:0.1032, test_correct0.9772\n","[103,   185] loss: 0.044  train_acc:0.9851\n","test_loss:0.1108, test_correct0.9742\n","[104,   185] loss: 0.040  train_acc:0.9866\n","test_loss:0.0847, test_correct0.9802\n","[105,   185] loss: 0.043  train_acc:0.9858\n","test_loss:0.1035, test_correct0.9650\n","[106,   185] loss: 0.046  train_acc:0.9858\n","test_loss:0.0958, test_correct0.9787\n","[107,   185] loss: 0.045  train_acc:0.9841\n","test_loss:0.0873, test_correct0.9787\n","[108,   185] loss: 0.047  train_acc:0.9851\n","test_loss:0.0780, test_correct0.9757\n","[109,   185] loss: 0.041  train_acc:0.9856\n","test_loss:0.0960, test_correct0.9787\n","[110,   185] loss: 0.041  train_acc:0.9875\n","test_loss:0.0948, test_correct0.9818\n","[111,   185] loss: 0.042  train_acc:0.9851\n","test_loss:0.0917, test_correct0.9742\n","[112,   185] loss: 0.043  train_acc:0.9851\n","test_loss:0.0935, test_correct0.9772\n","[113,   185] loss: 0.040  train_acc:0.9856\n","test_loss:0.1025, test_correct0.9772\n","[114,   185] loss: 0.044  train_acc:0.9860\n","test_loss:0.0968, test_correct0.9802\n","[115,   185] loss: 0.045  train_acc:0.9855\n","test_loss:0.0853, test_correct0.9742\n","[116,   185] loss: 0.038  train_acc:0.9882\n","test_loss:0.0946, test_correct0.9802\n","[117,   185] loss: 0.038  train_acc:0.9874\n","test_loss:0.0938, test_correct0.9772\n","[118,   185] loss: 0.043  train_acc:0.9841\n","test_loss:0.0927, test_correct0.9787\n","[119,   185] loss: 0.044  train_acc:0.9841\n","test_loss:0.0874, test_correct0.9772\n","[120,   185] loss: 0.039  train_acc:0.9880\n","test_loss:0.1049, test_correct0.9802\n","[121,   185] loss: 0.040  train_acc:0.9863\n","test_loss:0.1105, test_correct0.9787\n","[122,   185] loss: 0.045  train_acc:0.9841\n","test_loss:0.1001, test_correct0.9818\n","[123,   185] loss: 0.039  train_acc:0.9880\n","test_loss:0.0956, test_correct0.9772\n","[124,   185] loss: 0.043  train_acc:0.9865\n","test_loss:0.0951, test_correct0.9772\n","[125,   185] loss: 0.037  train_acc:0.9870\n","test_loss:0.0921, test_correct0.9818\n","[126,   185] loss: 0.046  train_acc:0.9856\n","test_loss:0.0877, test_correct0.9802\n","[127,   185] loss: 0.038  train_acc:0.9856\n","test_loss:0.0860, test_correct0.9802\n","[128,   185] loss: 0.041  train_acc:0.9854\n","test_loss:0.0959, test_correct0.9757\n","[129,   185] loss: 0.035  train_acc:0.9880\n","test_loss:0.0943, test_correct0.9757\n","[130,   185] loss: 0.039  train_acc:0.9870\n","test_loss:0.1176, test_correct0.9757\n","[131,   185] loss: 0.044  train_acc:0.9851\n","test_loss:0.1007, test_correct0.9772\n","[132,   185] loss: 0.041  train_acc:0.9875\n","test_loss:0.1048, test_correct0.9787\n","[133,   185] loss: 0.037  train_acc:0.9876\n","test_loss:0.0894, test_correct0.9818\n","[134,   185] loss: 0.045  train_acc:0.9843\n","test_loss:0.0932, test_correct0.9802\n","[135,   185] loss: 0.043  train_acc:0.9850\n","test_loss:0.0915, test_correct0.9818\n","[136,   185] loss: 0.040  train_acc:0.9858\n","test_loss:0.0922, test_correct0.9802\n","[137,   185] loss: 0.037  train_acc:0.9873\n","test_loss:0.1016, test_correct0.9802\n","[138,   185] loss: 0.044  train_acc:0.9866\n","test_loss:0.0803, test_correct0.9818\n","[139,   185] loss: 0.038  train_acc:0.9872\n","test_loss:0.0895, test_correct0.9726\n","[140,   185] loss: 0.038  train_acc:0.9868\n","test_loss:0.0944, test_correct0.9802\n","[141,   185] loss: 0.036  train_acc:0.9872\n","test_loss:0.0902, test_correct0.9787\n","[142,   185] loss: 0.037  train_acc:0.9868\n","test_loss:0.1037, test_correct0.9711\n","[143,   185] loss: 0.035  train_acc:0.9873\n","test_loss:0.1113, test_correct0.9711\n","[144,   185] loss: 0.039  train_acc:0.9865\n","test_loss:0.1011, test_correct0.9787\n","[145,   185] loss: 0.035  train_acc:0.9877\n","test_loss:0.1071, test_correct0.9726\n","[146,   185] loss: 0.036  train_acc:0.9863\n","test_loss:0.1107, test_correct0.9696\n","[147,   185] loss: 0.039  train_acc:0.9854\n","test_loss:0.0979, test_correct0.9711\n","[148,   185] loss: 0.036  train_acc:0.9878\n","test_loss:0.0855, test_correct0.9787\n","[149,   185] loss: 0.039  train_acc:0.9865\n","test_loss:0.0992, test_correct0.9726\n","[150,   185] loss: 0.036  train_acc:0.9882\n","test_loss:0.0860, test_correct0.9787\n","[151,   185] loss: 0.037  train_acc:0.9882\n","test_loss:0.1240, test_correct0.9681\n","[152,   185] loss: 0.036  train_acc:0.9880\n","test_loss:0.1067, test_correct0.9772\n","[153,   185] loss: 0.036  train_acc:0.9866\n","test_loss:0.1323, test_correct0.9696\n","[154,   185] loss: 0.035  train_acc:0.9869\n","test_loss:0.1275, test_correct0.9666\n","[155,   185] loss: 0.033  train_acc:0.9888\n","test_loss:0.1043, test_correct0.9787\n","[156,   185] loss: 0.032  train_acc:0.9888\n","test_loss:0.1300, test_correct0.9772\n","[157,   185] loss: 0.033  train_acc:0.9888\n","test_loss:0.1277, test_correct0.9666\n","[158,   185] loss: 0.037  train_acc:0.9861\n","test_loss:0.1287, test_correct0.9757\n","[159,   185] loss: 0.039  train_acc:0.9866\n","test_loss:0.1044, test_correct0.9742\n","[160,   185] loss: 0.032  train_acc:0.9888\n","test_loss:0.0911, test_correct0.9757\n","[161,   185] loss: 0.038  train_acc:0.9866\n","test_loss:0.1297, test_correct0.9650\n","[162,   185] loss: 0.037  train_acc:0.9880\n","test_loss:0.1047, test_correct0.9711\n","[163,   185] loss: 0.035  train_acc:0.9888\n","test_loss:0.1324, test_correct0.9726\n","[164,   185] loss: 0.036  train_acc:0.9870\n","test_loss:0.1196, test_correct0.9757\n","[165,   185] loss: 0.039  train_acc:0.9865\n","test_loss:0.1035, test_correct0.9757\n","[166,   185] loss: 0.036  train_acc:0.9880\n","test_loss:0.1085, test_correct0.9772\n","[167,   185] loss: 0.037  train_acc:0.9863\n","test_loss:0.0977, test_correct0.9726\n","[168,   185] loss: 0.032  train_acc:0.9885\n","test_loss:0.0985, test_correct0.9772\n","[169,   185] loss: 0.037  train_acc:0.9876\n","test_loss:0.0977, test_correct0.9726\n","[170,   185] loss: 0.033  train_acc:0.9892\n","test_loss:0.1075, test_correct0.9726\n","[171,   185] loss: 0.033  train_acc:0.9895\n","test_loss:0.1020, test_correct0.9757\n","[172,   185] loss: 0.036  train_acc:0.9880\n","test_loss:0.1026, test_correct0.9726\n","[173,   185] loss: 0.032  train_acc:0.9876\n","test_loss:0.1188, test_correct0.9726\n","[174,   185] loss: 0.032  train_acc:0.9898\n","test_loss:0.0968, test_correct0.9772\n","[175,   185] loss: 0.033  train_acc:0.9899\n","test_loss:0.1100, test_correct0.9726\n","[176,   185] loss: 0.036  train_acc:0.9880\n","test_loss:0.1127, test_correct0.9726\n","[177,   185] loss: 0.036  train_acc:0.9876\n","test_loss:0.1346, test_correct0.9666\n","[178,   185] loss: 0.036  train_acc:0.9882\n","test_loss:0.1341, test_correct0.9696\n","[179,   185] loss: 0.029  train_acc:0.9908\n","test_loss:0.1148, test_correct0.9726\n","[180,   185] loss: 0.031  train_acc:0.9897\n","test_loss:0.1216, test_correct0.9757\n","[181,   185] loss: 0.030  train_acc:0.9912\n","test_loss:0.1363, test_correct0.9711\n","[182,   185] loss: 0.035  train_acc:0.9885\n","test_loss:0.1202, test_correct0.9726\n","[183,   185] loss: 0.030  train_acc:0.9899\n","test_loss:0.1135, test_correct0.9742\n","[184,   185] loss: 0.032  train_acc:0.9902\n","test_loss:0.1057, test_correct0.9742\n","[185,   185] loss: 0.031  train_acc:0.9897\n","test_loss:0.1073, test_correct0.9742\n","[186,   185] loss: 0.034  train_acc:0.9877\n","test_loss:0.1317, test_correct0.9711\n","[187,   185] loss: 0.028  train_acc:0.9905\n","test_loss:0.1543, test_correct0.9681\n","[188,   185] loss: 0.028  train_acc:0.9907\n","test_loss:0.1350, test_correct0.9742\n","[189,   185] loss: 0.031  train_acc:0.9887\n","test_loss:0.1226, test_correct0.9711\n","[190,   185] loss: 0.032  train_acc:0.9887\n","test_loss:0.1099, test_correct0.9742\n","[191,   185] loss: 0.029  train_acc:0.9909\n","test_loss:0.1196, test_correct0.9726\n","[192,   185] loss: 0.028  train_acc:0.9922\n","test_loss:0.1209, test_correct0.9772\n","[193,   185] loss: 0.032  train_acc:0.9890\n","test_loss:0.1153, test_correct0.9742\n","[194,   185] loss: 0.028  train_acc:0.9915\n","test_loss:0.1200, test_correct0.9757\n","[195,   185] loss: 0.028  train_acc:0.9929\n","test_loss:0.1396, test_correct0.9742\n","[196,   185] loss: 0.029  train_acc:0.9899\n","test_loss:0.1244, test_correct0.9772\n","[197,   185] loss: 0.033  train_acc:0.9883\n","test_loss:0.1379, test_correct0.9742\n","[198,   185] loss: 0.032  train_acc:0.9880\n","test_loss:0.1282, test_correct0.9772\n","[199,   185] loss: 0.031  train_acc:0.9887\n","test_loss:0.1212, test_correct0.9726\n","[200,   185] loss: 0.034  train_acc:0.9897\n","test_loss:0.1560, test_correct0.9757\n","[1,   185] loss: 0.299  train_acc:0.8725\n","test_loss:0.3483, test_correct0.8799\n","[2,   185] loss: 0.128  train_acc:0.9518\n","test_loss:0.1153, test_correct0.9742\n","[3,   185] loss: 0.108  train_acc:0.9633\n","test_loss:0.1393, test_correct0.9650\n","[4,   185] loss: 0.095  train_acc:0.9669\n","test_loss:0.0969, test_correct0.9726\n","[5,   185] loss: 0.093  train_acc:0.9672\n","test_loss:0.1294, test_correct0.9681\n","[6,   185] loss: 0.097  train_acc:0.9645\n","test_loss:0.1139, test_correct0.9726\n","[7,   185] loss: 0.083  train_acc:0.9706\n","test_loss:0.0777, test_correct0.9818\n","[8,   185] loss: 0.086  train_acc:0.9716\n","test_loss:0.1269, test_correct0.9650\n","[9,   185] loss: 0.082  train_acc:0.9738\n","test_loss:0.1190, test_correct0.9681\n","[10,   185] loss: 0.085  train_acc:0.9723\n","test_loss:0.1058, test_correct0.9742\n","[11,   185] loss: 0.085  train_acc:0.9694\n","test_loss:0.1178, test_correct0.9726\n","[12,   185] loss: 0.073  train_acc:0.9756\n","test_loss:0.0952, test_correct0.9757\n","[13,   185] loss: 0.083  train_acc:0.9716\n","test_loss:0.0947, test_correct0.9742\n","[14,   185] loss: 0.073  train_acc:0.9740\n","test_loss:0.0802, test_correct0.9772\n","[15,   185] loss: 0.077  train_acc:0.9757\n","test_loss:0.0984, test_correct0.9726\n","[16,   185] loss: 0.073  train_acc:0.9734\n","test_loss:0.0958, test_correct0.9757\n","[17,   185] loss: 0.077  train_acc:0.9728\n","test_loss:0.1066, test_correct0.9742\n","[18,   185] loss: 0.067  train_acc:0.9777\n","test_loss:0.0917, test_correct0.9757\n","[19,   185] loss: 0.070  train_acc:0.9773\n","test_loss:0.0825, test_correct0.9772\n","[20,   185] loss: 0.071  train_acc:0.9760\n","test_loss:0.1006, test_correct0.9757\n","[21,   185] loss: 0.064  train_acc:0.9777\n","test_loss:0.0977, test_correct0.9742\n","[22,   185] loss: 0.066  train_acc:0.9748\n","test_loss:0.1017, test_correct0.9696\n","[23,   185] loss: 0.068  train_acc:0.9758\n","test_loss:0.0939, test_correct0.9802\n","[24,   185] loss: 0.070  train_acc:0.9750\n","test_loss:0.0955, test_correct0.9772\n","[25,   185] loss: 0.071  train_acc:0.9748\n","test_loss:0.0952, test_correct0.9787\n","[26,   185] loss: 0.069  train_acc:0.9770\n","test_loss:0.0907, test_correct0.9742\n","[27,   185] loss: 0.067  train_acc:0.9773\n","test_loss:0.0825, test_correct0.9818\n","[28,   185] loss: 0.064  train_acc:0.9781\n","test_loss:0.1080, test_correct0.9757\n","[29,   185] loss: 0.066  train_acc:0.9773\n","test_loss:0.0889, test_correct0.9802\n","[30,   185] loss: 0.066  train_acc:0.9777\n","test_loss:0.1055, test_correct0.9726\n","[31,   185] loss: 0.062  train_acc:0.9775\n","test_loss:0.1113, test_correct0.9787\n","[32,   185] loss: 0.066  train_acc:0.9779\n","test_loss:0.0730, test_correct0.9787\n","[33,   185] loss: 0.064  train_acc:0.9799\n","test_loss:0.0845, test_correct0.9787\n","[34,   185] loss: 0.060  train_acc:0.9799\n","test_loss:0.0814, test_correct0.9787\n","[35,   185] loss: 0.063  train_acc:0.9804\n","test_loss:0.1013, test_correct0.9772\n","[36,   185] loss: 0.064  train_acc:0.9785\n","test_loss:0.1212, test_correct0.9787\n","[37,   185] loss: 0.058  train_acc:0.9794\n","test_loss:0.1251, test_correct0.9757\n","[38,   185] loss: 0.059  train_acc:0.9794\n","test_loss:0.1062, test_correct0.9757\n","[39,   185] loss: 0.065  train_acc:0.9771\n","test_loss:0.1312, test_correct0.9711\n","[40,   185] loss: 0.059  train_acc:0.9807\n","test_loss:0.1462, test_correct0.9711\n","[41,   185] loss: 0.058  train_acc:0.9807\n","test_loss:0.1412, test_correct0.9696\n","[42,   185] loss: 0.059  train_acc:0.9790\n","test_loss:0.1141, test_correct0.9787\n","[43,   185] loss: 0.060  train_acc:0.9797\n","test_loss:0.1162, test_correct0.9742\n","[44,   185] loss: 0.059  train_acc:0.9799\n","test_loss:0.1236, test_correct0.9787\n","[45,   185] loss: 0.059  train_acc:0.9773\n","test_loss:0.0926, test_correct0.9818\n","[46,   185] loss: 0.062  train_acc:0.9788\n","test_loss:0.0991, test_correct0.9802\n","[47,   185] loss: 0.058  train_acc:0.9814\n","test_loss:0.1349, test_correct0.9742\n","[48,   185] loss: 0.058  train_acc:0.9811\n","test_loss:0.1438, test_correct0.9711\n","[49,   185] loss: 0.058  train_acc:0.9795\n","test_loss:0.0936, test_correct0.9802\n","[50,   185] loss: 0.061  train_acc:0.9814\n","test_loss:0.1132, test_correct0.9757\n","[51,   185] loss: 0.056  train_acc:0.9810\n","test_loss:0.1289, test_correct0.9711\n","[52,   185] loss: 0.053  train_acc:0.9816\n","test_loss:0.1008, test_correct0.9802\n","[53,   185] loss: 0.055  train_acc:0.9821\n","test_loss:0.1232, test_correct0.9711\n","[54,   185] loss: 0.057  train_acc:0.9787\n","test_loss:0.1109, test_correct0.9802\n","[55,   185] loss: 0.056  train_acc:0.9802\n","test_loss:0.1286, test_correct0.9742\n","[56,   185] loss: 0.059  train_acc:0.9783\n","test_loss:0.1210, test_correct0.9726\n","[57,   185] loss: 0.055  train_acc:0.9811\n","test_loss:0.0995, test_correct0.9802\n","[58,   185] loss: 0.060  train_acc:0.9781\n","test_loss:0.1063, test_correct0.9726\n","[59,   185] loss: 0.053  train_acc:0.9823\n","test_loss:0.1217, test_correct0.9726\n","[60,   185] loss: 0.060  train_acc:0.9801\n","test_loss:0.0964, test_correct0.9787\n","[61,   185] loss: 0.055  train_acc:0.9802\n","test_loss:0.0993, test_correct0.9787\n","[62,   185] loss: 0.052  train_acc:0.9812\n","test_loss:0.1030, test_correct0.9772\n","[63,   185] loss: 0.055  train_acc:0.9807\n","test_loss:0.1600, test_correct0.9681\n","[64,   185] loss: 0.053  train_acc:0.9811\n","test_loss:0.1103, test_correct0.9787\n","[65,   185] loss: 0.055  train_acc:0.9807\n","test_loss:0.1243, test_correct0.9711\n","[66,   185] loss: 0.055  train_acc:0.9798\n","test_loss:0.1578, test_correct0.9635\n","[67,   185] loss: 0.051  train_acc:0.9819\n","test_loss:0.1198, test_correct0.9711\n","[68,   185] loss: 0.054  train_acc:0.9824\n","test_loss:0.1021, test_correct0.9818\n","[69,   185] loss: 0.053  train_acc:0.9809\n","test_loss:0.1276, test_correct0.9711\n","[70,   185] loss: 0.050  train_acc:0.9822\n","test_loss:0.1045, test_correct0.9711\n","[71,   185] loss: 0.053  train_acc:0.9788\n","test_loss:0.1172, test_correct0.9711\n","[72,   185] loss: 0.051  train_acc:0.9826\n","test_loss:0.1118, test_correct0.9802\n","[73,   185] loss: 0.051  train_acc:0.9817\n","test_loss:0.1496, test_correct0.9711\n","[74,   185] loss: 0.051  train_acc:0.9830\n","test_loss:0.1847, test_correct0.9696\n","[75,   185] loss: 0.050  train_acc:0.9821\n","test_loss:0.1099, test_correct0.9772\n","[76,   185] loss: 0.058  train_acc:0.9794\n","test_loss:0.1274, test_correct0.9711\n","[77,   185] loss: 0.054  train_acc:0.9819\n","test_loss:0.1919, test_correct0.9650\n","[78,   185] loss: 0.050  train_acc:0.9816\n","test_loss:0.1149, test_correct0.9742\n","[79,   185] loss: 0.046  train_acc:0.9838\n","test_loss:0.1579, test_correct0.9711\n","[80,   185] loss: 0.050  train_acc:0.9828\n","test_loss:0.0883, test_correct0.9757\n","[81,   185] loss: 0.050  train_acc:0.9826\n","test_loss:0.1335, test_correct0.9726\n","[82,   185] loss: 0.048  train_acc:0.9824\n","test_loss:0.1248, test_correct0.9742\n","[83,   185] loss: 0.055  train_acc:0.9811\n","test_loss:0.1260, test_correct0.9726\n","[84,   185] loss: 0.050  train_acc:0.9814\n","test_loss:0.1053, test_correct0.9726\n","[85,   185] loss: 0.047  train_acc:0.9834\n","test_loss:0.1095, test_correct0.9742\n","[86,   185] loss: 0.054  train_acc:0.9807\n","test_loss:0.1224, test_correct0.9742\n","[87,   185] loss: 0.049  train_acc:0.9827\n","test_loss:0.1237, test_correct0.9757\n","[88,   185] loss: 0.048  train_acc:0.9832\n","test_loss:0.1105, test_correct0.9757\n","[89,   185] loss: 0.048  train_acc:0.9830\n","test_loss:0.1139, test_correct0.9772\n","[90,   185] loss: 0.047  train_acc:0.9841\n","test_loss:0.0995, test_correct0.9726\n","[91,   185] loss: 0.044  train_acc:0.9851\n","test_loss:0.1006, test_correct0.9742\n","[92,   185] loss: 0.049  train_acc:0.9817\n","test_loss:0.1241, test_correct0.9726\n","[93,   185] loss: 0.047  train_acc:0.9850\n","test_loss:0.1591, test_correct0.9681\n","[94,   185] loss: 0.050  train_acc:0.9824\n","test_loss:0.1164, test_correct0.9742\n","[95,   185] loss: 0.046  train_acc:0.9824\n","test_loss:0.0952, test_correct0.9772\n","[96,   185] loss: 0.049  train_acc:0.9844\n","test_loss:0.1249, test_correct0.9726\n","[97,   185] loss: 0.049  train_acc:0.9822\n","test_loss:0.1617, test_correct0.9726\n","[98,   185] loss: 0.045  train_acc:0.9841\n","test_loss:0.0892, test_correct0.9802\n","[99,   185] loss: 0.047  train_acc:0.9839\n","test_loss:0.1037, test_correct0.9757\n","[100,   185] loss: 0.045  train_acc:0.9854\n","test_loss:0.1041, test_correct0.9787\n","[101,   185] loss: 0.048  train_acc:0.9817\n","test_loss:0.0982, test_correct0.9757\n","[102,   185] loss: 0.047  train_acc:0.9849\n","test_loss:0.1263, test_correct0.9772\n","[103,   185] loss: 0.052  train_acc:0.9824\n","test_loss:0.1282, test_correct0.9742\n","[104,   185] loss: 0.047  train_acc:0.9843\n","test_loss:0.0998, test_correct0.9787\n","[105,   185] loss: 0.043  train_acc:0.9836\n","test_loss:0.0909, test_correct0.9757\n","[106,   185] loss: 0.043  train_acc:0.9848\n","test_loss:0.0890, test_correct0.9757\n","[107,   185] loss: 0.043  train_acc:0.9871\n","test_loss:0.1050, test_correct0.9802\n","[108,   185] loss: 0.044  train_acc:0.9846\n","test_loss:0.0886, test_correct0.9757\n","[109,   185] loss: 0.045  train_acc:0.9849\n","test_loss:0.1017, test_correct0.9772\n","[110,   185] loss: 0.041  train_acc:0.9846\n","test_loss:0.1013, test_correct0.9711\n","[111,   185] loss: 0.042  train_acc:0.9875\n","test_loss:0.1009, test_correct0.9742\n","[112,   185] loss: 0.046  train_acc:0.9833\n","test_loss:0.1029, test_correct0.9772\n","[113,   185] loss: 0.043  train_acc:0.9853\n","test_loss:0.1210, test_correct0.9757\n","[114,   185] loss: 0.044  train_acc:0.9851\n","test_loss:0.1095, test_correct0.9742\n","[115,   185] loss: 0.042  train_acc:0.9846\n","test_loss:0.0889, test_correct0.9772\n","[116,   185] loss: 0.042  train_acc:0.9853\n","test_loss:0.1123, test_correct0.9802\n","[117,   185] loss: 0.041  train_acc:0.9866\n","test_loss:0.0873, test_correct0.9818\n","[118,   185] loss: 0.040  train_acc:0.9866\n","test_loss:0.1072, test_correct0.9772\n","[119,   185] loss: 0.043  train_acc:0.9848\n","test_loss:0.1180, test_correct0.9757\n","[120,   185] loss: 0.046  train_acc:0.9851\n","test_loss:0.1063, test_correct0.9772\n","[121,   185] loss: 0.044  train_acc:0.9861\n","test_loss:0.1161, test_correct0.9772\n","[122,   185] loss: 0.043  train_acc:0.9856\n","test_loss:0.1141, test_correct0.9787\n","[123,   185] loss: 0.040  train_acc:0.9870\n","test_loss:0.1247, test_correct0.9772\n","[124,   185] loss: 0.044  train_acc:0.9849\n","test_loss:0.1111, test_correct0.9757\n","[125,   185] loss: 0.039  train_acc:0.9872\n","test_loss:0.0996, test_correct0.9757\n","[126,   185] loss: 0.043  train_acc:0.9851\n","test_loss:0.0896, test_correct0.9802\n","[127,   185] loss: 0.039  train_acc:0.9855\n","test_loss:0.1098, test_correct0.9757\n","[128,   185] loss: 0.040  train_acc:0.9870\n","test_loss:0.0977, test_correct0.9787\n","[129,   185] loss: 0.038  train_acc:0.9877\n","test_loss:0.1153, test_correct0.9757\n","[130,   185] loss: 0.037  train_acc:0.9883\n","test_loss:0.1036, test_correct0.9772\n","[131,   185] loss: 0.041  train_acc:0.9856\n","test_loss:0.1566, test_correct0.9726\n","[132,   185] loss: 0.040  train_acc:0.9871\n","test_loss:0.1096, test_correct0.9772\n","[133,   185] loss: 0.039  train_acc:0.9866\n","test_loss:0.1251, test_correct0.9742\n","[134,   185] loss: 0.039  train_acc:0.9860\n","test_loss:0.1038, test_correct0.9787\n","[135,   185] loss: 0.038  train_acc:0.9853\n","test_loss:0.0912, test_correct0.9802\n","[136,   185] loss: 0.039  train_acc:0.9866\n","test_loss:0.0995, test_correct0.9802\n","[137,   185] loss: 0.035  train_acc:0.9882\n","test_loss:0.1024, test_correct0.9802\n","[138,   185] loss: 0.037  train_acc:0.9881\n","test_loss:0.1008, test_correct0.9802\n","[139,   185] loss: 0.038  train_acc:0.9878\n","test_loss:0.1224, test_correct0.9742\n","[140,   185] loss: 0.041  train_acc:0.9866\n","test_loss:0.1006, test_correct0.9787\n","[141,   185] loss: 0.038  train_acc:0.9887\n","test_loss:0.1013, test_correct0.9833\n","[142,   185] loss: 0.041  train_acc:0.9861\n","test_loss:0.1297, test_correct0.9757\n","[143,   185] loss: 0.038  train_acc:0.9888\n","test_loss:0.1391, test_correct0.9787\n","[144,   185] loss: 0.038  train_acc:0.9876\n","test_loss:0.1145, test_correct0.9772\n","[145,   185] loss: 0.041  train_acc:0.9849\n","test_loss:0.1046, test_correct0.9802\n","[146,   185] loss: 0.040  train_acc:0.9868\n","test_loss:0.1093, test_correct0.9742\n","[147,   185] loss: 0.036  train_acc:0.9868\n","test_loss:0.1088, test_correct0.9818\n","[148,   185] loss: 0.040  train_acc:0.9872\n","test_loss:0.1398, test_correct0.9757\n","[149,   185] loss: 0.038  train_acc:0.9878\n","test_loss:0.1422, test_correct0.9742\n","[150,   185] loss: 0.037  train_acc:0.9863\n","test_loss:0.1156, test_correct0.9802\n","[151,   185] loss: 0.042  train_acc:0.9858\n","test_loss:0.1773, test_correct0.9742\n","[152,   185] loss: 0.038  train_acc:0.9887\n","test_loss:0.1193, test_correct0.9802\n","[153,   185] loss: 0.039  train_acc:0.9872\n","test_loss:0.0957, test_correct0.9787\n","[154,   185] loss: 0.037  train_acc:0.9878\n","test_loss:0.1065, test_correct0.9833\n","[155,   185] loss: 0.037  train_acc:0.9873\n","test_loss:0.1343, test_correct0.9848\n","[156,   185] loss: 0.042  train_acc:0.9848\n","test_loss:0.1548, test_correct0.9772\n","[157,   185] loss: 0.035  train_acc:0.9890\n","test_loss:0.1286, test_correct0.9772\n","[158,   185] loss: 0.036  train_acc:0.9865\n","test_loss:0.1643, test_correct0.9757\n","[159,   185] loss: 0.036  train_acc:0.9883\n","test_loss:0.1572, test_correct0.9802\n","[160,   185] loss: 0.039  train_acc:0.9885\n","test_loss:0.1081, test_correct0.9787\n","[161,   185] loss: 0.036  train_acc:0.9866\n","test_loss:0.1369, test_correct0.9848\n","[162,   185] loss: 0.034  train_acc:0.9875\n","test_loss:0.1157, test_correct0.9818\n","[163,   185] loss: 0.040  train_acc:0.9871\n","test_loss:0.1333, test_correct0.9818\n","[164,   185] loss: 0.035  train_acc:0.9888\n","test_loss:0.1378, test_correct0.9818\n","[165,   185] loss: 0.032  train_acc:0.9900\n","test_loss:0.1254, test_correct0.9863\n","[166,   185] loss: 0.032  train_acc:0.9902\n","test_loss:0.1328, test_correct0.9802\n","[167,   185] loss: 0.034  train_acc:0.9883\n","test_loss:0.1829, test_correct0.9757\n","[168,   185] loss: 0.041  train_acc:0.9878\n","test_loss:0.1204, test_correct0.9848\n","[169,   185] loss: 0.033  train_acc:0.9887\n","test_loss:0.1064, test_correct0.9833\n","[170,   185] loss: 0.033  train_acc:0.9903\n","test_loss:0.1360, test_correct0.9802\n","[171,   185] loss: 0.032  train_acc:0.9883\n","test_loss:0.1264, test_correct0.9848\n","[172,   185] loss: 0.036  train_acc:0.9883\n","test_loss:0.1204, test_correct0.9848\n","[173,   185] loss: 0.034  train_acc:0.9887\n","test_loss:0.1249, test_correct0.9818\n","[174,   185] loss: 0.034  train_acc:0.9887\n","test_loss:0.1266, test_correct0.9818\n","[175,   185] loss: 0.032  train_acc:0.9892\n","test_loss:0.0969, test_correct0.9818\n","[176,   185] loss: 0.037  train_acc:0.9875\n","test_loss:0.1564, test_correct0.9742\n","[177,   185] loss: 0.035  train_acc:0.9887\n","test_loss:0.1302, test_correct0.9757\n","[178,   185] loss: 0.036  train_acc:0.9885\n","test_loss:0.1364, test_correct0.9848\n","[179,   185] loss: 0.033  train_acc:0.9885\n","test_loss:0.1091, test_correct0.9802\n","[180,   185] loss: 0.030  train_acc:0.9905\n","test_loss:0.1333, test_correct0.9848\n","[181,   185] loss: 0.034  train_acc:0.9892\n","test_loss:0.1182, test_correct0.9848\n","[182,   185] loss: 0.036  train_acc:0.9880\n","test_loss:0.1814, test_correct0.9818\n","[183,   185] loss: 0.033  train_acc:0.9883\n","test_loss:0.2039, test_correct0.7857\n","[184,   185] loss: 0.033  train_acc:0.9887\n","test_loss:0.1028, test_correct0.9863\n","[185,   185] loss: 0.041  train_acc:0.9880\n","test_loss:0.1097, test_correct0.9848\n","[186,   185] loss: 0.033  train_acc:0.9890\n","test_loss:0.1005, test_correct0.9818\n","[187,   185] loss: 0.029  train_acc:0.9905\n","test_loss:0.1069, test_correct0.9863\n","[188,   185] loss: 0.029  train_acc:0.9887\n","test_loss:0.1260, test_correct0.9772\n","[189,   185] loss: 0.032  train_acc:0.9900\n","test_loss:0.1341, test_correct0.9833\n","[190,   185] loss: 0.036  train_acc:0.9873\n","test_loss:0.1027, test_correct0.9833\n","[191,   185] loss: 0.033  train_acc:0.9883\n","test_loss:0.1364, test_correct0.9787\n","[192,   185] loss: 0.032  train_acc:0.9909\n","test_loss:0.1159, test_correct0.9802\n","[193,   185] loss: 0.031  train_acc:0.9903\n","test_loss:0.1496, test_correct0.9742\n","[194,   185] loss: 0.029  train_acc:0.9897\n","test_loss:0.1478, test_correct0.9833\n","[195,   185] loss: 0.030  train_acc:0.9900\n","test_loss:0.1767, test_correct0.9818\n","[196,   185] loss: 0.033  train_acc:0.9894\n","test_loss:0.1844, test_correct0.9711\n","[197,   185] loss: 0.029  train_acc:0.9900\n","test_loss:0.1537, test_correct0.9833\n","[198,   185] loss: 0.024  train_acc:0.9922\n","test_loss:0.1642, test_correct0.9863\n","[199,   185] loss: 0.031  train_acc:0.9892\n","test_loss:0.1665, test_correct0.9818\n","[200,   185] loss: 0.027  train_acc:0.9910\n","test_loss:0.1354, test_correct0.9802\n","[1,   185] loss: 0.303  train_acc:0.8844\n","test_loss:0.7225, test_correct0.8146\n","[2,   185] loss: 0.138  train_acc:0.9460\n","test_loss:0.4713, test_correct0.8419\n","[3,   185] loss: 0.108  train_acc:0.9597\n","test_loss:0.2222, test_correct0.9422\n","[4,   185] loss: 0.096  train_acc:0.9658\n","test_loss:0.2130, test_correct0.9438\n","[5,   185] loss: 0.089  train_acc:0.9677\n","test_loss:0.2106, test_correct0.9483\n","[6,   185] loss: 0.088  train_acc:0.9696\n","test_loss:0.2028, test_correct0.9514\n","[7,   185] loss: 0.086  train_acc:0.9684\n","test_loss:0.1579, test_correct0.9574\n","[8,   185] loss: 0.085  train_acc:0.9682\n","test_loss:0.1575, test_correct0.9544\n","[9,   185] loss: 0.082  train_acc:0.9724\n","test_loss:0.1243, test_correct0.9559\n","[10,   185] loss: 0.073  train_acc:0.9733\n","test_loss:0.1451, test_correct0.9605\n","[11,   185] loss: 0.078  train_acc:0.9721\n","test_loss:0.1862, test_correct0.9544\n","[12,   185] loss: 0.077  train_acc:0.9735\n","test_loss:0.1309, test_correct0.9650\n","[13,   185] loss: 0.072  train_acc:0.9750\n","test_loss:0.1347, test_correct0.9650\n","[14,   185] loss: 0.075  train_acc:0.9741\n","test_loss:0.1309, test_correct0.9666\n","[15,   185] loss: 0.068  train_acc:0.9771\n","test_loss:0.1181, test_correct0.9635\n","[16,   185] loss: 0.069  train_acc:0.9766\n","test_loss:0.1319, test_correct0.9681\n","[17,   185] loss: 0.070  train_acc:0.9745\n","test_loss:0.1229, test_correct0.9666\n","[18,   185] loss: 0.067  train_acc:0.9753\n","test_loss:0.1078, test_correct0.9696\n","[19,   185] loss: 0.071  train_acc:0.9748\n","test_loss:0.1223, test_correct0.9650\n","[20,   185] loss: 0.067  train_acc:0.9780\n","test_loss:0.1159, test_correct0.9681\n","[21,   185] loss: 0.066  train_acc:0.9748\n","test_loss:0.1167, test_correct0.9681\n","[22,   185] loss: 0.067  train_acc:0.9753\n","test_loss:0.1066, test_correct0.9711\n","[23,   185] loss: 0.064  train_acc:0.9784\n","test_loss:0.1215, test_correct0.9666\n","[24,   185] loss: 0.061  train_acc:0.9780\n","test_loss:0.1107, test_correct0.9620\n","[25,   185] loss: 0.065  train_acc:0.9794\n","test_loss:0.1135, test_correct0.9650\n","[26,   185] loss: 0.066  train_acc:0.9789\n","test_loss:0.1257, test_correct0.9666\n","[27,   185] loss: 0.059  train_acc:0.9777\n","test_loss:0.1223, test_correct0.9635\n","[28,   185] loss: 0.061  train_acc:0.9801\n","test_loss:0.1190, test_correct0.9681\n","[29,   185] loss: 0.060  train_acc:0.9785\n","test_loss:0.1119, test_correct0.9620\n","[30,   185] loss: 0.061  train_acc:0.9792\n","test_loss:0.1236, test_correct0.9635\n","[31,   185] loss: 0.062  train_acc:0.9775\n","test_loss:0.1199, test_correct0.9635\n","[32,   185] loss: 0.059  train_acc:0.9785\n","test_loss:0.1427, test_correct0.9650\n","[33,   185] loss: 0.060  train_acc:0.9802\n","test_loss:0.1191, test_correct0.9681\n","[34,   185] loss: 0.061  train_acc:0.9797\n","test_loss:0.1272, test_correct0.9650\n","[35,   185] loss: 0.059  train_acc:0.9802\n","test_loss:0.1315, test_correct0.9635\n","[36,   185] loss: 0.064  train_acc:0.9755\n","test_loss:0.1426, test_correct0.9681\n","[37,   185] loss: 0.056  train_acc:0.9817\n","test_loss:0.1157, test_correct0.9711\n","[38,   185] loss: 0.056  train_acc:0.9777\n","test_loss:0.1245, test_correct0.9711\n","[39,   185] loss: 0.054  train_acc:0.9797\n","test_loss:0.1091, test_correct0.9696\n","[40,   185] loss: 0.057  train_acc:0.9807\n","test_loss:0.1286, test_correct0.9681\n","[41,   185] loss: 0.055  train_acc:0.9812\n","test_loss:0.1083, test_correct0.9711\n","[42,   185] loss: 0.051  train_acc:0.9803\n","test_loss:0.1266, test_correct0.9681\n","[43,   185] loss: 0.058  train_acc:0.9795\n","test_loss:0.1157, test_correct0.9696\n","[44,   185] loss: 0.055  train_acc:0.9809\n","test_loss:0.1144, test_correct0.9711\n","[45,   185] loss: 0.054  train_acc:0.9816\n","test_loss:0.1311, test_correct0.9650\n","[46,   185] loss: 0.051  train_acc:0.9826\n","test_loss:0.1167, test_correct0.9696\n","[47,   185] loss: 0.057  train_acc:0.9823\n","test_loss:0.1186, test_correct0.9711\n","[48,   185] loss: 0.049  train_acc:0.9824\n","test_loss:0.1175, test_correct0.9666\n","[49,   185] loss: 0.057  train_acc:0.9807\n","test_loss:0.1146, test_correct0.9696\n","[50,   185] loss: 0.049  train_acc:0.9829\n","test_loss:0.1108, test_correct0.9681\n","[51,   185] loss: 0.052  train_acc:0.9810\n","test_loss:0.1242, test_correct0.9635\n","[52,   185] loss: 0.054  train_acc:0.9804\n","test_loss:0.1104, test_correct0.9681\n","[53,   185] loss: 0.054  train_acc:0.9806\n","test_loss:0.1198, test_correct0.9650\n","[54,   185] loss: 0.054  train_acc:0.9810\n","test_loss:0.1164, test_correct0.9696\n","[55,   185] loss: 0.052  train_acc:0.9812\n","test_loss:0.1182, test_correct0.9696\n","[56,   185] loss: 0.051  train_acc:0.9819\n","test_loss:0.1135, test_correct0.9666\n","[57,   185] loss: 0.058  train_acc:0.9779\n","test_loss:0.1286, test_correct0.9620\n","[58,   185] loss: 0.051  train_acc:0.9827\n","test_loss:0.1234, test_correct0.9666\n","[59,   185] loss: 0.048  train_acc:0.9829\n","test_loss:0.1317, test_correct0.9650\n","[60,   185] loss: 0.054  train_acc:0.9801\n","test_loss:0.1236, test_correct0.9666\n","[61,   185] loss: 0.049  train_acc:0.9817\n","test_loss:0.1316, test_correct0.9650\n","[62,   185] loss: 0.051  train_acc:0.9834\n","test_loss:0.1273, test_correct0.9666\n","[63,   185] loss: 0.047  train_acc:0.9824\n","test_loss:0.1217, test_correct0.9635\n","[64,   185] loss: 0.052  train_acc:0.9798\n","test_loss:0.1166, test_correct0.9666\n","[65,   185] loss: 0.049  train_acc:0.9831\n","test_loss:0.1155, test_correct0.9635\n","[66,   185] loss: 0.048  train_acc:0.9829\n","test_loss:0.1241, test_correct0.9681\n","[67,   185] loss: 0.047  train_acc:0.9824\n","test_loss:0.1275, test_correct0.9666\n","[68,   185] loss: 0.046  train_acc:0.9843\n","test_loss:0.1302, test_correct0.9666\n","[69,   185] loss: 0.047  train_acc:0.9844\n","test_loss:0.1134, test_correct0.9650\n","[70,   185] loss: 0.049  train_acc:0.9836\n","test_loss:0.1326, test_correct0.9650\n","[71,   185] loss: 0.046  train_acc:0.9828\n","test_loss:0.1170, test_correct0.9650\n","[72,   185] loss: 0.050  train_acc:0.9802\n","test_loss:0.1238, test_correct0.9666\n","[73,   185] loss: 0.046  train_acc:0.9838\n","test_loss:0.1242, test_correct0.9666\n","[74,   185] loss: 0.046  train_acc:0.9850\n","test_loss:0.1234, test_correct0.9696\n","[75,   185] loss: 0.047  train_acc:0.9838\n","test_loss:0.1211, test_correct0.9650\n","[76,   185] loss: 0.042  train_acc:0.9855\n","test_loss:0.1252, test_correct0.9681\n","[77,   185] loss: 0.045  train_acc:0.9853\n","test_loss:0.1244, test_correct0.9681\n","[78,   185] loss: 0.053  train_acc:0.9814\n","test_loss:0.1408, test_correct0.9635\n","[79,   185] loss: 0.047  train_acc:0.9827\n","test_loss:0.1198, test_correct0.9666\n","[80,   185] loss: 0.041  train_acc:0.9863\n","test_loss:0.1276, test_correct0.9666\n","[81,   185] loss: 0.045  train_acc:0.9824\n","test_loss:0.1225, test_correct0.9681\n","[82,   185] loss: 0.047  train_acc:0.9849\n","test_loss:0.1347, test_correct0.9681\n","[83,   185] loss: 0.046  train_acc:0.9846\n","test_loss:0.1474, test_correct0.9666\n","[84,   185] loss: 0.045  train_acc:0.9834\n","test_loss:0.1253, test_correct0.9666\n","[85,   185] loss: 0.044  train_acc:0.9824\n","test_loss:0.1399, test_correct0.9666\n","[86,   185] loss: 0.045  train_acc:0.9843\n","test_loss:0.1261, test_correct0.9650\n","[87,   185] loss: 0.045  train_acc:0.9834\n","test_loss:0.1263, test_correct0.9666\n","[88,   185] loss: 0.045  train_acc:0.9836\n","test_loss:0.1361, test_correct0.9666\n","[89,   185] loss: 0.045  train_acc:0.9834\n","test_loss:0.1262, test_correct0.9666\n","[90,   185] loss: 0.046  train_acc:0.9855\n","test_loss:0.1258, test_correct0.9666\n","[91,   185] loss: 0.044  train_acc:0.9848\n","test_loss:0.1220, test_correct0.9650\n","[92,   185] loss: 0.044  train_acc:0.9851\n","test_loss:0.1205, test_correct0.9635\n","[93,   185] loss: 0.043  train_acc:0.9841\n","test_loss:0.1272, test_correct0.9666\n","[94,   185] loss: 0.044  train_acc:0.9849\n","test_loss:0.1308, test_correct0.9681\n","[95,   185] loss: 0.048  train_acc:0.9814\n","test_loss:0.1306, test_correct0.9681\n","[96,   185] loss: 0.047  train_acc:0.9851\n","test_loss:0.1310, test_correct0.9681\n","[97,   185] loss: 0.042  train_acc:0.9846\n","test_loss:0.1206, test_correct0.9696\n","[98,   185] loss: 0.041  train_acc:0.9851\n","test_loss:0.1208, test_correct0.9650\n","[99,   185] loss: 0.040  train_acc:0.9868\n","test_loss:0.1301, test_correct0.9681\n","[100,   185] loss: 0.043  train_acc:0.9868\n","test_loss:0.1173, test_correct0.9666\n","[101,   185] loss: 0.036  train_acc:0.9874\n","test_loss:0.1218, test_correct0.9666\n","[102,   185] loss: 0.038  train_acc:0.9861\n","test_loss:0.1336, test_correct0.9666\n","[103,   185] loss: 0.043  train_acc:0.9832\n","test_loss:0.1294, test_correct0.9666\n","[104,   185] loss: 0.044  train_acc:0.9845\n","test_loss:0.1308, test_correct0.9666\n","[105,   185] loss: 0.046  train_acc:0.9841\n","test_loss:0.1265, test_correct0.9681\n","[106,   185] loss: 0.040  train_acc:0.9853\n","test_loss:0.1417, test_correct0.9696\n","[107,   185] loss: 0.045  train_acc:0.9824\n","test_loss:0.1294, test_correct0.9666\n","[108,   185] loss: 0.038  train_acc:0.9878\n","test_loss:0.1328, test_correct0.9681\n","[109,   185] loss: 0.045  train_acc:0.9843\n","test_loss:0.1263, test_correct0.9681\n","[110,   185] loss: 0.041  train_acc:0.9865\n","test_loss:0.1230, test_correct0.9681\n","[111,   185] loss: 0.044  train_acc:0.9836\n","test_loss:0.1288, test_correct0.9666\n","[112,   185] loss: 0.040  train_acc:0.9851\n","test_loss:0.1386, test_correct0.9650\n","[113,   185] loss: 0.038  train_acc:0.9882\n","test_loss:0.1417, test_correct0.9666\n","[114,   185] loss: 0.037  train_acc:0.9873\n","test_loss:0.1321, test_correct0.9635\n","[115,   185] loss: 0.042  train_acc:0.9856\n","test_loss:0.1409, test_correct0.9666\n","[116,   185] loss: 0.042  train_acc:0.9854\n","test_loss:0.1286, test_correct0.9681\n","[117,   185] loss: 0.042  train_acc:0.9841\n","test_loss:0.1178, test_correct0.9650\n","[118,   185] loss: 0.040  train_acc:0.9855\n","test_loss:0.1378, test_correct0.9666\n","[119,   185] loss: 0.038  train_acc:0.9861\n","test_loss:0.1427, test_correct0.9696\n","[120,   185] loss: 0.036  train_acc:0.9873\n","test_loss:0.1226, test_correct0.9635\n","[121,   185] loss: 0.038  train_acc:0.9866\n","test_loss:0.1262, test_correct0.9650\n","[122,   185] loss: 0.035  train_acc:0.9876\n","test_loss:0.1338, test_correct0.9681\n","[123,   185] loss: 0.041  train_acc:0.9860\n","test_loss:0.1316, test_correct0.9666\n","[124,   185] loss: 0.038  train_acc:0.9865\n","test_loss:0.1385, test_correct0.9666\n","[125,   185] loss: 0.037  train_acc:0.9873\n","test_loss:0.1384, test_correct0.9635\n","[126,   185] loss: 0.035  train_acc:0.9887\n","test_loss:0.1289, test_correct0.9666\n","[127,   185] loss: 0.036  train_acc:0.9870\n","test_loss:0.1343, test_correct0.9681\n","[128,   185] loss: 0.039  train_acc:0.9880\n","test_loss:0.1381, test_correct0.9650\n","[129,   185] loss: 0.037  train_acc:0.9871\n","test_loss:0.1268, test_correct0.9666\n","[130,   185] loss: 0.037  train_acc:0.9870\n","test_loss:0.1469, test_correct0.9650\n","[131,   185] loss: 0.036  train_acc:0.9871\n","test_loss:0.1613, test_correct0.9681\n","[132,   185] loss: 0.034  train_acc:0.9885\n","test_loss:0.1400, test_correct0.9635\n","[133,   185] loss: 0.035  train_acc:0.9890\n","test_loss:0.1256, test_correct0.9681\n","[134,   185] loss: 0.039  train_acc:0.9863\n","test_loss:0.1722, test_correct0.9590\n","[135,   185] loss: 0.045  train_acc:0.9861\n","test_loss:0.1270, test_correct0.9650\n","[136,   185] loss: 0.041  train_acc:0.9855\n","test_loss:0.1219, test_correct0.9666\n","[137,   185] loss: 0.036  train_acc:0.9875\n","test_loss:0.2067, test_correct0.9422\n","[138,   185] loss: 0.041  train_acc:0.9861\n","test_loss:0.1602, test_correct0.9590\n","[139,   185] loss: 0.037  train_acc:0.9871\n","test_loss:0.1562, test_correct0.9620\n","[140,   185] loss: 0.033  train_acc:0.9880\n","test_loss:0.1496, test_correct0.9620\n","[141,   185] loss: 0.037  train_acc:0.9865\n","test_loss:0.1215, test_correct0.9666\n","[142,   185] loss: 0.033  train_acc:0.9894\n","test_loss:0.1330, test_correct0.9650\n","[143,   185] loss: 0.033  train_acc:0.9894\n","test_loss:0.1371, test_correct0.9681\n","[144,   185] loss: 0.042  train_acc:0.9870\n","test_loss:0.1386, test_correct0.9620\n","[145,   185] loss: 0.032  train_acc:0.9873\n","test_loss:0.1370, test_correct0.9650\n","[146,   185] loss: 0.037  train_acc:0.9885\n","test_loss:0.1426, test_correct0.9666\n","[147,   185] loss: 0.034  train_acc:0.9878\n","test_loss:0.1406, test_correct0.9681\n","[148,   185] loss: 0.032  train_acc:0.9894\n","test_loss:0.1446, test_correct0.9635\n","[149,   185] loss: 0.031  train_acc:0.9900\n","test_loss:0.1650, test_correct0.9666\n","[150,   185] loss: 0.035  train_acc:0.9886\n","test_loss:0.1431, test_correct0.9711\n","[151,   185] loss: 0.033  train_acc:0.9883\n","test_loss:0.1413, test_correct0.9620\n","[152,   185] loss: 0.033  train_acc:0.9893\n","test_loss:0.1514, test_correct0.9696\n","[153,   185] loss: 0.045  train_acc:0.9868\n","test_loss:0.1508, test_correct0.9666\n","[154,   185] loss: 0.034  train_acc:0.9892\n","test_loss:0.1583, test_correct0.9650\n","[155,   185] loss: 0.033  train_acc:0.9887\n","test_loss:0.1407, test_correct0.9696\n","[156,   185] loss: 0.036  train_acc:0.9880\n","test_loss:0.1545, test_correct0.9650\n","[157,   185] loss: 0.030  train_acc:0.9907\n","test_loss:0.1418, test_correct0.9681\n","[158,   185] loss: 0.034  train_acc:0.9877\n","test_loss:0.1426, test_correct0.9681\n","[159,   185] loss: 0.036  train_acc:0.9868\n","test_loss:0.1399, test_correct0.9605\n","[160,   185] loss: 0.032  train_acc:0.9907\n","test_loss:0.1478, test_correct0.9650\n","[161,   185] loss: 0.032  train_acc:0.9885\n","test_loss:0.1403, test_correct0.9635\n","[162,   185] loss: 0.033  train_acc:0.9902\n","test_loss:0.1531, test_correct0.9696\n","[163,   185] loss: 0.031  train_acc:0.9897\n","test_loss:0.1501, test_correct0.9681\n","[164,   185] loss: 0.031  train_acc:0.9888\n","test_loss:0.1371, test_correct0.9711\n","[165,   185] loss: 0.031  train_acc:0.9905\n","test_loss:0.1254, test_correct0.9696\n","[166,   185] loss: 0.033  train_acc:0.9885\n","test_loss:0.1435, test_correct0.9666\n","[167,   185] loss: 0.030  train_acc:0.9893\n","test_loss:0.1300, test_correct0.9635\n","[168,   185] loss: 0.031  train_acc:0.9895\n","test_loss:0.1408, test_correct0.9696\n","[169,   185] loss: 0.030  train_acc:0.9901\n","test_loss:0.1439, test_correct0.9681\n","[170,   185] loss: 0.031  train_acc:0.9893\n","test_loss:0.1167, test_correct0.9681\n","[171,   185] loss: 0.033  train_acc:0.9890\n","test_loss:0.1274, test_correct0.9696\n","[172,   185] loss: 0.031  train_acc:0.9909\n","test_loss:0.1387, test_correct0.9696\n","[173,   185] loss: 0.034  train_acc:0.9864\n","test_loss:0.1315, test_correct0.9620\n","[174,   185] loss: 0.033  train_acc:0.9883\n","test_loss:0.1734, test_correct0.9605\n","[175,   185] loss: 0.032  train_acc:0.9887\n","test_loss:0.1440, test_correct0.9666\n","[176,   185] loss: 0.028  train_acc:0.9914\n","test_loss:0.1408, test_correct0.9696\n","[177,   185] loss: 0.034  train_acc:0.9877\n","test_loss:0.1385, test_correct0.9696\n","[178,   185] loss: 0.032  train_acc:0.9897\n","test_loss:0.1399, test_correct0.9711\n","[179,   185] loss: 0.030  train_acc:0.9904\n","test_loss:0.1437, test_correct0.9681\n","[180,   185] loss: 0.027  train_acc:0.9915\n","test_loss:0.1463, test_correct0.9681\n","[181,   185] loss: 0.030  train_acc:0.9888\n","test_loss:0.1307, test_correct0.9681\n","[182,   185] loss: 0.027  train_acc:0.9900\n","test_loss:0.1330, test_correct0.9696\n","[183,   185] loss: 0.033  train_acc:0.9895\n","test_loss:0.1292, test_correct0.9681\n","[184,   185] loss: 0.030  train_acc:0.9898\n","test_loss:0.1633, test_correct0.9620\n","[185,   185] loss: 0.029  train_acc:0.9900\n","test_loss:0.1399, test_correct0.9666\n","[186,   185] loss: 0.030  train_acc:0.9887\n","test_loss:0.1848, test_correct0.9590\n","[187,   185] loss: 0.038  train_acc:0.9866\n","test_loss:0.1441, test_correct0.9666\n","[188,   185] loss: 0.032  train_acc:0.9894\n","test_loss:0.1476, test_correct0.9666\n","[189,   185] loss: 0.026  train_acc:0.9917\n","test_loss:0.1584, test_correct0.9635\n","[190,   185] loss: 0.026  train_acc:0.9929\n","test_loss:0.1471, test_correct0.9605\n","[191,   185] loss: 0.027  train_acc:0.9905\n","test_loss:0.1723, test_correct0.9590\n","[192,   185] loss: 0.031  train_acc:0.9902\n","test_loss:0.1461, test_correct0.9590\n","[193,   185] loss: 0.040  train_acc:0.9878\n","test_loss:0.1498, test_correct0.9666\n","[194,   185] loss: 0.028  train_acc:0.9899\n","test_loss:0.1419, test_correct0.9696\n","[195,   185] loss: 0.029  train_acc:0.9910\n","test_loss:0.1470, test_correct0.9696\n","[196,   185] loss: 0.027  train_acc:0.9912\n","test_loss:0.1445, test_correct0.9681\n","[197,   185] loss: 0.030  train_acc:0.9897\n","test_loss:0.1192, test_correct0.9711\n","[198,   185] loss: 0.029  train_acc:0.9898\n","test_loss:0.1752, test_correct0.9559\n","[199,   185] loss: 0.026  train_acc:0.9908\n","test_loss:0.1529, test_correct0.9620\n","[200,   185] loss: 0.032  train_acc:0.9892\n","test_loss:0.1316, test_correct0.9635\n","[1,   185] loss: 0.296  train_acc:0.8766\n","test_loss:0.4823, test_correct0.8571\n","[2,   185] loss: 0.122  train_acc:0.9535\n","test_loss:0.2739, test_correct0.9195\n","[3,   185] loss: 0.106  train_acc:0.9596\n","test_loss:0.2352, test_correct0.9286\n","[4,   185] loss: 0.099  train_acc:0.9660\n","test_loss:0.2045, test_correct0.9407\n","[5,   185] loss: 0.092  train_acc:0.9672\n","test_loss:0.2202, test_correct0.9422\n","[6,   185] loss: 0.092  train_acc:0.9667\n","test_loss:0.1963, test_correct0.9483\n","[7,   185] loss: 0.089  train_acc:0.9682\n","test_loss:0.1849, test_correct0.9468\n","[8,   185] loss: 0.088  train_acc:0.9682\n","test_loss:0.1525, test_correct0.9620\n","[9,   185] loss: 0.082  train_acc:0.9724\n","test_loss:0.1383, test_correct0.9635\n","[10,   185] loss: 0.086  train_acc:0.9687\n","test_loss:0.1665, test_correct0.9544\n","[11,   185] loss: 0.080  train_acc:0.9702\n","test_loss:0.1019, test_correct0.9711\n","[12,   185] loss: 0.084  train_acc:0.9730\n","test_loss:0.1412, test_correct0.9635\n","[13,   185] loss: 0.078  train_acc:0.9721\n","test_loss:0.1455, test_correct0.9666\n","[14,   185] loss: 0.078  train_acc:0.9740\n","test_loss:0.1410, test_correct0.9620\n","[15,   185] loss: 0.080  train_acc:0.9708\n","test_loss:0.1086, test_correct0.9696\n","[16,   185] loss: 0.078  train_acc:0.9706\n","test_loss:0.1442, test_correct0.9605\n","[17,   185] loss: 0.079  train_acc:0.9733\n","test_loss:0.1382, test_correct0.9620\n","[18,   185] loss: 0.071  train_acc:0.9750\n","test_loss:0.1647, test_correct0.9620\n","[19,   185] loss: 0.072  train_acc:0.9773\n","test_loss:0.0991, test_correct0.9681\n","[20,   185] loss: 0.071  train_acc:0.9733\n","test_loss:0.1219, test_correct0.9635\n","[21,   185] loss: 0.073  train_acc:0.9748\n","test_loss:0.1191, test_correct0.9696\n","[22,   185] loss: 0.069  train_acc:0.9746\n","test_loss:0.1400, test_correct0.9681\n","[23,   185] loss: 0.066  train_acc:0.9780\n","test_loss:0.1316, test_correct0.9696\n","[24,   185] loss: 0.068  train_acc:0.9756\n","test_loss:0.1803, test_correct0.9529\n","[25,   185] loss: 0.072  train_acc:0.9752\n","test_loss:0.0976, test_correct0.9742\n","[26,   185] loss: 0.070  train_acc:0.9758\n","test_loss:0.1080, test_correct0.9726\n","[27,   185] loss: 0.072  train_acc:0.9733\n","test_loss:0.1233, test_correct0.9711\n","[28,   185] loss: 0.066  train_acc:0.9763\n","test_loss:0.1346, test_correct0.9681\n","[29,   185] loss: 0.066  train_acc:0.9772\n","test_loss:0.1288, test_correct0.9711\n","[30,   185] loss: 0.065  train_acc:0.9765\n","test_loss:0.1561, test_correct0.9666\n","[31,   185] loss: 0.064  train_acc:0.9782\n","test_loss:0.1208, test_correct0.9757\n","[32,   185] loss: 0.062  train_acc:0.9782\n","test_loss:0.1479, test_correct0.9650\n","[33,   185] loss: 0.067  train_acc:0.9763\n","test_loss:0.1243, test_correct0.9711\n","[34,   185] loss: 0.061  train_acc:0.9788\n","test_loss:0.1258, test_correct0.9726\n","[35,   185] loss: 0.066  train_acc:0.9765\n","test_loss:0.1781, test_correct0.9666\n","[36,   185] loss: 0.059  train_acc:0.9785\n","test_loss:0.1388, test_correct0.9696\n","[37,   185] loss: 0.060  train_acc:0.9775\n","test_loss:0.1501, test_correct0.9757\n","[38,   185] loss: 0.061  train_acc:0.9792\n","test_loss:0.1700, test_correct0.9650\n","[39,   185] loss: 0.063  train_acc:0.9780\n","test_loss:0.1323, test_correct0.9726\n","[40,   185] loss: 0.063  train_acc:0.9785\n","test_loss:0.1674, test_correct0.9650\n","[41,   185] loss: 0.058  train_acc:0.9805\n","test_loss:0.1591, test_correct0.9650\n","[42,   185] loss: 0.061  train_acc:0.9780\n","test_loss:0.1337, test_correct0.9757\n","[43,   185] loss: 0.065  train_acc:0.9784\n","test_loss:0.1493, test_correct0.9650\n","[44,   185] loss: 0.061  train_acc:0.9783\n","test_loss:0.1716, test_correct0.9696\n","[45,   185] loss: 0.059  train_acc:0.9785\n","test_loss:0.1391, test_correct0.9696\n","[46,   185] loss: 0.059  train_acc:0.9799\n","test_loss:0.1525, test_correct0.9696\n","[47,   185] loss: 0.055  train_acc:0.9809\n","test_loss:0.1990, test_correct0.9726\n","[48,   185] loss: 0.059  train_acc:0.9787\n","test_loss:0.1671, test_correct0.9681\n","[49,   185] loss: 0.059  train_acc:0.9788\n","test_loss:0.1611, test_correct0.9742\n","[50,   185] loss: 0.057  train_acc:0.9802\n","test_loss:0.1510, test_correct0.9772\n","[51,   185] loss: 0.058  train_acc:0.9782\n","test_loss:0.1616, test_correct0.9711\n","[52,   185] loss: 0.059  train_acc:0.9802\n","test_loss:0.1644, test_correct0.9757\n","[53,   185] loss: 0.058  train_acc:0.9810\n","test_loss:0.1812, test_correct0.9711\n","[54,   185] loss: 0.057  train_acc:0.9801\n","test_loss:0.1770, test_correct0.9696\n","[55,   185] loss: 0.054  train_acc:0.9806\n","test_loss:0.2100, test_correct0.9666\n","[56,   185] loss: 0.057  train_acc:0.9807\n","test_loss:0.1848, test_correct0.9726\n","[57,   185] loss: 0.056  train_acc:0.9804\n","test_loss:0.2144, test_correct0.7918\n","[58,   185] loss: 0.059  train_acc:0.9797\n","test_loss:0.1860, test_correct0.9711\n","[59,   185] loss: 0.057  train_acc:0.9816\n","test_loss:0.1644, test_correct0.9742\n","[60,   185] loss: 0.054  train_acc:0.9810\n","test_loss:0.1907, test_correct0.9696\n","[61,   185] loss: 0.057  train_acc:0.9794\n","test_loss:0.2216, test_correct0.9696\n","[62,   185] loss: 0.056  train_acc:0.9823\n","test_loss:0.1866, test_correct0.9726\n","[63,   185] loss: 0.055  train_acc:0.9795\n","test_loss:0.2011, test_correct0.9681\n","[64,   185] loss: 0.056  train_acc:0.9810\n","test_loss:0.1956, test_correct0.9696\n","[65,   185] loss: 0.054  train_acc:0.9816\n","test_loss:0.1872, test_correct0.9711\n","[66,   185] loss: 0.052  train_acc:0.9816\n","test_loss:0.1953, test_correct0.9681\n","[67,   185] loss: 0.058  train_acc:0.9802\n","test_loss:0.1663, test_correct0.9696\n","[68,   185] loss: 0.055  train_acc:0.9817\n","test_loss:0.1904, test_correct0.9726\n","[69,   185] loss: 0.056  train_acc:0.9802\n","test_loss:0.2034, test_correct0.9681\n","[70,   185] loss: 0.052  train_acc:0.9821\n","test_loss:0.1709, test_correct0.9787\n","[71,   185] loss: 0.052  train_acc:0.9827\n","test_loss:0.1763, test_correct0.9711\n","[72,   185] loss: 0.052  train_acc:0.9824\n","test_loss:0.1946, test_correct0.9711\n","[73,   185] loss: 0.050  train_acc:0.9831\n","test_loss:0.1769, test_correct0.9742\n","[74,   185] loss: 0.052  train_acc:0.9841\n","test_loss:0.1869, test_correct0.9726\n","[75,   185] loss: 0.049  train_acc:0.9837\n","test_loss:0.2645, test_correct0.7903\n","[76,   185] loss: 0.049  train_acc:0.9827\n","test_loss:0.1928, test_correct0.9711\n","[77,   185] loss: 0.054  train_acc:0.9807\n","test_loss:0.1763, test_correct0.9742\n","[78,   185] loss: 0.054  train_acc:0.9795\n","test_loss:0.1895, test_correct0.9742\n","[79,   185] loss: 0.060  train_acc:0.9807\n","test_loss:0.1611, test_correct0.9726\n","[80,   185] loss: 0.049  train_acc:0.9824\n","test_loss:0.1816, test_correct0.9711\n","[81,   185] loss: 0.051  train_acc:0.9827\n","test_loss:0.1849, test_correct0.9711\n","[82,   185] loss: 0.053  train_acc:0.9814\n","test_loss:0.1905, test_correct0.9711\n","[83,   185] loss: 0.056  train_acc:0.9809\n","test_loss:0.1844, test_correct0.9726\n","[84,   185] loss: 0.052  train_acc:0.9816\n","test_loss:0.1808, test_correct0.9726\n","[85,   185] loss: 0.052  train_acc:0.9839\n","test_loss:0.1875, test_correct0.7948\n","[86,   185] loss: 0.051  train_acc:0.9839\n","test_loss:0.1841, test_correct0.9742\n","[87,   185] loss: 0.053  train_acc:0.9790\n","test_loss:0.2014, test_correct0.7933\n","[88,   185] loss: 0.050  train_acc:0.9843\n","test_loss:0.1823, test_correct0.9757\n","[89,   185] loss: 0.048  train_acc:0.9841\n","test_loss:0.1952, test_correct0.7964\n","[90,   185] loss: 0.047  train_acc:0.9826\n","test_loss:0.2111, test_correct0.7948\n","[91,   185] loss: 0.048  train_acc:0.9819\n","test_loss:0.2022, test_correct0.7948\n","[92,   185] loss: 0.046  train_acc:0.9853\n","test_loss:0.1970, test_correct0.9757\n","[93,   185] loss: 0.048  train_acc:0.9861\n","test_loss:0.1834, test_correct0.9726\n","[94,   185] loss: 0.046  train_acc:0.9846\n","test_loss:0.1885, test_correct0.9696\n","[95,   185] loss: 0.047  train_acc:0.9833\n","test_loss:0.1962, test_correct0.9742\n","[96,   185] loss: 0.050  train_acc:0.9839\n","test_loss:0.1928, test_correct0.7979\n","[97,   185] loss: 0.048  train_acc:0.9844\n","test_loss:0.2299, test_correct0.7918\n","[98,   185] loss: 0.047  train_acc:0.9860\n","test_loss:0.2046, test_correct0.7933\n","[99,   185] loss: 0.047  train_acc:0.9846\n","test_loss:0.1809, test_correct0.9772\n","[100,   185] loss: 0.050  train_acc:0.9828\n","test_loss:0.1870, test_correct0.8040\n","[101,   185] loss: 0.047  train_acc:0.9839\n","test_loss:0.1989, test_correct0.7964\n","[102,   185] loss: 0.046  train_acc:0.9851\n","test_loss:0.1994, test_correct0.7948\n","[103,   185] loss: 0.049  train_acc:0.9826\n","test_loss:0.2035, test_correct0.7964\n","[104,   185] loss: 0.047  train_acc:0.9841\n","test_loss:0.2177, test_correct0.7933\n","[105,   185] loss: 0.048  train_acc:0.9846\n","test_loss:0.2116, test_correct0.7933\n","[106,   185] loss: 0.046  train_acc:0.9856\n","test_loss:0.2107, test_correct0.7964\n","[107,   185] loss: 0.046  train_acc:0.9838\n","test_loss:0.2050, test_correct0.7994\n","[108,   185] loss: 0.049  train_acc:0.9843\n","test_loss:0.2289, test_correct0.7933\n","[109,   185] loss: 0.046  train_acc:0.9839\n","test_loss:0.2272, test_correct0.7979\n","[110,   185] loss: 0.045  train_acc:0.9824\n","test_loss:0.2063, test_correct0.7964\n","[111,   185] loss: 0.049  train_acc:0.9839\n","test_loss:0.2252, test_correct0.7918\n","[112,   185] loss: 0.046  train_acc:0.9841\n","test_loss:0.2163, test_correct0.7933\n","[113,   185] loss: 0.043  train_acc:0.9853\n","test_loss:0.2362, test_correct0.7979\n","[114,   185] loss: 0.044  train_acc:0.9861\n","test_loss:0.2406, test_correct0.7933\n","[115,   185] loss: 0.054  train_acc:0.9824\n","test_loss:0.2445, test_correct0.7918\n","[116,   185] loss: 0.051  train_acc:0.9831\n","test_loss:0.2012, test_correct0.7933\n","[117,   185] loss: 0.045  train_acc:0.9851\n","test_loss:0.2130, test_correct0.7948\n","[118,   185] loss: 0.045  train_acc:0.9853\n","test_loss:0.1971, test_correct0.7948\n","[119,   185] loss: 0.041  train_acc:0.9848\n","test_loss:0.2062, test_correct0.7948\n","[120,   185] loss: 0.045  train_acc:0.9861\n","test_loss:0.2262, test_correct0.7933\n","[121,   185] loss: 0.043  train_acc:0.9841\n","test_loss:0.2188, test_correct0.7964\n","[122,   185] loss: 0.043  train_acc:0.9850\n","test_loss:0.2588, test_correct0.7933\n","[123,   185] loss: 0.044  train_acc:0.9865\n","test_loss:0.2271, test_correct0.7979\n","[124,   185] loss: 0.046  train_acc:0.9846\n","test_loss:0.2084, test_correct0.7948\n","[125,   185] loss: 0.046  train_acc:0.9850\n","test_loss:0.2076, test_correct0.9696\n","[126,   185] loss: 0.039  train_acc:0.9871\n","test_loss:0.1969, test_correct0.7964\n","[127,   185] loss: 0.040  train_acc:0.9868\n","test_loss:0.2016, test_correct0.7933\n","[128,   185] loss: 0.044  train_acc:0.9865\n","test_loss:0.2190, test_correct0.7948\n","[129,   185] loss: 0.042  train_acc:0.9866\n","test_loss:0.2055, test_correct0.7964\n","[130,   185] loss: 0.042  train_acc:0.9863\n","test_loss:0.2153, test_correct0.7964\n","[131,   185] loss: 0.048  train_acc:0.9849\n","test_loss:0.2346, test_correct0.8009\n","[132,   185] loss: 0.040  train_acc:0.9878\n","test_loss:0.2212, test_correct0.8009\n","[133,   185] loss: 0.041  train_acc:0.9870\n","test_loss:0.2354, test_correct0.7994\n","[134,   185] loss: 0.041  train_acc:0.9873\n","test_loss:0.2024, test_correct0.7964\n","[135,   185] loss: 0.038  train_acc:0.9873\n","test_loss:0.2470, test_correct0.7948\n","[136,   185] loss: 0.043  train_acc:0.9873\n","test_loss:0.2296, test_correct0.7964\n","[137,   185] loss: 0.040  train_acc:0.9866\n","test_loss:0.2333, test_correct0.7948\n","[138,   185] loss: 0.039  train_acc:0.9870\n","test_loss:0.2320, test_correct0.7979\n","[139,   185] loss: 0.039  train_acc:0.9875\n","test_loss:0.2323, test_correct0.7994\n","[140,   185] loss: 0.042  train_acc:0.9853\n","test_loss:0.2329, test_correct0.7994\n","[141,   185] loss: 0.043  train_acc:0.9868\n","test_loss:0.2150, test_correct0.8009\n","[142,   185] loss: 0.038  train_acc:0.9871\n","test_loss:0.2113, test_correct0.7964\n","[143,   185] loss: 0.039  train_acc:0.9880\n","test_loss:0.2254, test_correct0.7948\n","[144,   185] loss: 0.035  train_acc:0.9892\n","test_loss:0.2304, test_correct0.7979\n","[145,   185] loss: 0.036  train_acc:0.9882\n","test_loss:0.2489, test_correct0.7948\n","[146,   185] loss: 0.041  train_acc:0.9859\n","test_loss:0.2037, test_correct0.7994\n","[147,   185] loss: 0.039  train_acc:0.9873\n","test_loss:0.2057, test_correct0.9726\n","[148,   185] loss: 0.040  train_acc:0.9881\n","test_loss:0.1776, test_correct0.9772\n","[149,   185] loss: 0.038  train_acc:0.9877\n","test_loss:0.1970, test_correct0.9726\n","[150,   185] loss: 0.037  train_acc:0.9885\n","test_loss:0.1908, test_correct0.7948\n","[151,   185] loss: 0.038  train_acc:0.9880\n","test_loss:0.1911, test_correct0.7979\n","[152,   185] loss: 0.040  train_acc:0.9882\n","test_loss:0.1828, test_correct0.7979\n","[153,   185] loss: 0.038  train_acc:0.9875\n","test_loss:0.2081, test_correct0.7948\n","[154,   185] loss: 0.041  train_acc:0.9872\n","test_loss:0.2468, test_correct0.7933\n","[155,   185] loss: 0.035  train_acc:0.9883\n","test_loss:0.2032, test_correct0.7964\n","[156,   185] loss: 0.033  train_acc:0.9878\n","test_loss:0.2066, test_correct0.7948\n","[157,   185] loss: 0.034  train_acc:0.9885\n","test_loss:0.2027, test_correct0.7994\n","[158,   185] loss: 0.038  train_acc:0.9883\n","test_loss:0.1933, test_correct0.7933\n","[159,   185] loss: 0.039  train_acc:0.9863\n","test_loss:0.1998, test_correct0.7933\n","[160,   185] loss: 0.041  train_acc:0.9868\n","test_loss:0.2323, test_correct0.7948\n","[161,   185] loss: 0.035  train_acc:0.9900\n","test_loss:0.2311, test_correct0.7918\n","[162,   185] loss: 0.035  train_acc:0.9902\n","test_loss:0.2472, test_correct0.7888\n","[163,   185] loss: 0.031  train_acc:0.9905\n","test_loss:0.2258, test_correct0.7933\n","[164,   185] loss: 0.038  train_acc:0.9888\n","test_loss:0.2388, test_correct0.7933\n","[165,   185] loss: 0.036  train_acc:0.9877\n","test_loss:0.2083, test_correct0.7964\n","[166,   185] loss: 0.037  train_acc:0.9875\n","test_loss:0.2068, test_correct0.7964\n","[167,   185] loss: 0.038  train_acc:0.9875\n","test_loss:0.1875, test_correct0.9757\n","[168,   185] loss: 0.033  train_acc:0.9903\n","test_loss:0.1732, test_correct0.9711\n","[169,   185] loss: 0.036  train_acc:0.9866\n","test_loss:0.1921, test_correct0.9711\n","[170,   185] loss: 0.037  train_acc:0.9888\n","test_loss:0.1924, test_correct0.7964\n","[171,   185] loss: 0.034  train_acc:0.9875\n","test_loss:0.1862, test_correct0.9757\n","[172,   185] loss: 0.032  train_acc:0.9895\n","test_loss:0.2057, test_correct0.9696\n","[173,   185] loss: 0.035  train_acc:0.9877\n","test_loss:0.2043, test_correct0.9696\n","[174,   185] loss: 0.037  train_acc:0.9880\n","test_loss:0.2019, test_correct0.7948\n","[175,   185] loss: 0.034  train_acc:0.9881\n","test_loss:0.1815, test_correct0.9757\n","[176,   185] loss: 0.033  train_acc:0.9890\n","test_loss:0.1874, test_correct0.9711\n","[177,   185] loss: 0.036  train_acc:0.9895\n","test_loss:0.1730, test_correct0.9711\n","[178,   185] loss: 0.037  train_acc:0.9877\n","test_loss:0.1925, test_correct0.9726\n","[179,   185] loss: 0.033  train_acc:0.9897\n","test_loss:0.1762, test_correct0.9772\n","[180,   185] loss: 0.028  train_acc:0.9907\n","test_loss:0.1971, test_correct0.9742\n","[181,   185] loss: 0.039  train_acc:0.9877\n","test_loss:0.1788, test_correct0.9742\n","[182,   185] loss: 0.036  train_acc:0.9885\n","test_loss:0.1790, test_correct0.9742\n","[183,   185] loss: 0.034  train_acc:0.9890\n","test_loss:0.1787, test_correct0.9742\n","[184,   185] loss: 0.031  train_acc:0.9888\n","test_loss:0.1960, test_correct0.9726\n","[185,   185] loss: 0.035  train_acc:0.9882\n","test_loss:0.1736, test_correct0.9772\n","[186,   185] loss: 0.033  train_acc:0.9883\n","test_loss:0.1870, test_correct0.9711\n","[187,   185] loss: 0.032  train_acc:0.9902\n","test_loss:0.1816, test_correct0.9742\n","[188,   185] loss: 0.033  train_acc:0.9894\n","test_loss:0.1969, test_correct0.9650\n","[189,   185] loss: 0.033  train_acc:0.9894\n","test_loss:0.1869, test_correct0.9772\n","[190,   185] loss: 0.035  train_acc:0.9888\n","test_loss:0.2046, test_correct0.7948\n","[191,   185] loss: 0.032  train_acc:0.9887\n","test_loss:0.1695, test_correct0.9772\n","[192,   185] loss: 0.031  train_acc:0.9909\n","test_loss:0.1911, test_correct0.7964\n","[193,   185] loss: 0.033  train_acc:0.9893\n","test_loss:0.1987, test_correct0.9726\n","[194,   185] loss: 0.032  train_acc:0.9888\n","test_loss:0.1898, test_correct0.9726\n","[195,   185] loss: 0.028  train_acc:0.9920\n","test_loss:0.1947, test_correct0.9742\n","[196,   185] loss: 0.030  train_acc:0.9905\n","test_loss:0.1707, test_correct0.9726\n","[197,   185] loss: 0.032  train_acc:0.9885\n","test_loss:0.1681, test_correct0.9787\n","[198,   185] loss: 0.031  train_acc:0.9903\n","test_loss:0.1755, test_correct0.9742\n","[199,   185] loss: 0.029  train_acc:0.9907\n","test_loss:0.1924, test_correct0.9711\n","[200,   185] loss: 0.031  train_acc:0.9899\n","test_loss:0.1963, test_correct0.9711\n","[1,   185] loss: 0.296  train_acc:0.8794\n","test_loss:0.3751, test_correct0.8539\n","[2,   185] loss: 0.148  train_acc:0.9460\n","test_loss:0.4207, test_correct0.8539\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-d5f3fc2ce934>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0macc_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mlos_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlos_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlos_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0macc_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.85\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-7dd1ea3725cf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x, model, los_train, acc_train)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;31m# accracy =np.mean( (torch.argmax(output.cuda(),1)==torch.argmax(labels.cuda(),1)).numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mrunning_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\n","\n","import torch\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchsummary\n","import scipy.io as sio\n","from torch.utils.data import DataLoader,Dataset,TensorDataset\n","from torch.autograd import Variable\n","import numpy as np\n","import h5py\n","from sklearn.preprocessing import minmax_scale\n","import torch.optim as optim\n","from keras.utils import to_categorical\n","from sklearn.model_selection import StratifiedKFold\n","from scipy.fftpack import fft, ifft\n","import scipy.signal as signal\n","\n","# print(torch.cuda.is_available())\n","def zscore(data):\n","    data_mean=np.mean(data)\n","    data_std=np.std(data, axis=0)\n","    if data_std!=0:\n","        data=(data-data_mean)/data_std\n","    else:\n","        data=data-data_mean\n","    return data\n","def butthigh(ecg, fs):\n","    # wp = 1\n","    # ws = 0.5\n","    # Rp = 0.3\n","    # Rs = 2\n","    # [N1, Wn1] = signal.buttord(wp / (fs / 2), ws / (fs / 2), Rp, Rs)\n","    # [N1, Wn1] = signal.buttord(wp / (fs*1.0 / 2), ws / (fs*1.0 / 2), Rp, Rs)\n","\n","    # [b1, a1] = signal.butter(N1, Wn1, 'high')\n","    b1 = np.array([0.995155038209359, -1.99031007641872, 0.995155038209359])\n","    a1 = np.array([1, -1.99028660262621, 0.990333550211225])\n","    ecg_copy = np.copy(ecg)\n","    ecg1 = signal.filtfilt(b1, a1, ecg_copy)\n","    return ecg1\n","def hobalka(ecg1, fs, fmin, fmax):\n","    ecg = np.copy(ecg1)\n","    n = len(ecg)\n","    ecg_fil = fft(ecg)\n","    if fmin > 0:\n","        imin = int(fmin / (fs / n))\n","    else:\n","        imin = 1\n","        ecg_fil[0] = ecg_fil[0] / 2\n","    if fmax < fs / 2:\n","        imax = int(fmax / float(fs / n))\n","    else:\n","        imax = int(n / 2)\n","    hamwindow = np.hamming(imax - imin)\n","    hamsize = len(hamwindow)\n","    yy = np.zeros(len(ecg_fil), dtype=complex)\n","    istred = int((imax + imin) / 2)\n","    dolni = np.arange(istred-1, imax)\n","    ld = len(dolni)\n","    yy[0: ld] = np.multiply(ecg_fil[dolni - 1], hamwindow[int(np.floor(hamsize / 2)) - 1: hamsize])\n","    horni = np.arange(imin-1, istred-1)\n","    lh = len(horni)\n","    end = len(yy)\n","    yy[end - lh - 1: end - 1] = np.multiply(ecg_fil[horni], hamwindow[0: int(np.floor(hamsize / 2))])\n","    ecg_fil = abs(ifft(yy)) * 2\n","    return ecg_fil\n","#loader\n","class  conv1d_inception_block(nn.Module):\n","    \"\"\"\n","    Convolution Block 1d\n","    \"\"\"\n","    def __init__(self, in_ch, out_ch):\n","        super(conv1d_inception_block, self).__init__()\n","\n","        self.conv1_1 = nn.Sequential(\n","            nn.Conv1d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n","            nn.Dropout(0.3))\n","        self.conv1_3 = nn.Sequential(\n","            nn.Conv1d(in_ch, out_ch, kernel_size=5, stride=1, padding=2, bias=True),\n","            nn.Dropout(0.3))\n","        self.conv1_5 = nn.Sequential(\n","            nn.Conv1d(in_ch, out_ch, kernel_size=7, stride=1, padding=3, bias=True),\n","            nn.Dropout(0.3))\n","        self.conv= nn.Sequential(\n","            nn.Conv1d(in_ch, out_ch, kernel_size=1, stride=1, padding=0, bias=True),\n","            #nn.Conv1d(in_ch, out_ch, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.Dropout(0.3),\n","            nn.BatchNorm1d(out_ch),\n","            nn.ReLU())\n","    def forward(self, x):\n","\n","        x1 = self.conv1_1(x)\n","        x3 = self.conv1_3(x)\n","        x5 = self.conv1_5(x)\n","        return self.conv(x1+x3+x5)\n","class Recurrent_block(nn.Module):\n","    \"\"\"\n","    Recurrent Block for R2Unet_CNN\n","      '''\n","            conv1d_inception_block(out_ch,out_ch),\n","            nn.Dropout(0.2),\n","            nn.Conv1d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n","            nn.Dropout(0.2),\n","            '''\n","    \"\"\"\n","    def __init__(self, out_ch, t=2):\n","        super(Recurrent_block, self).__init__()\n","        #self.drop_layer = nn.Dropout(0.5)\n","        self.t = t\n","        self.out_ch = out_ch\n","        self.conv = nn.Sequential(\n","\n","            conv1d_inception_block(out_ch,out_ch),\n","            nn.Dropout(0.3),\n","            nn.BatchNorm1d(out_ch),\n","            nn.ReLU()\n","        )\n","        self.conv1_1 = nn.Sequential(\n","            nn.Conv1d(out_ch, out_ch, kernel_size=1, stride=1, padding=0, bias=True))\n","    def forward(self, x):\n","        for i in range(self.t):\n","            if i == 0:\n","                x1 = self.conv(x)\n","            out = self.conv1_1(x1 + x)\n","        return out\n","\n","class Residual_block(nn.Module):\n","    \"\"\"\n","    Recurrent Block for R2Unet_CNN\n","      '''\n","            conv1d_inception_block(out_ch,out_ch),\n","            nn.Dropout(0.2),\n","            nn.Conv1d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n","            nn.Dropout(0.2),\n","            '''\n","    \"\"\"\n","    def __init__(self, out_ch, t=2):\n","        super(Residual_block, self).__init__()\n","        #self.drop_layer = nn.Dropout(0.5)\n","        self.t = t\n","        self.out_ch = out_ch\n","        self.conv = nn.Sequential(\n","\n","            conv1d_inception_block(out_ch,out_ch),\n","            nn.Dropout(0.3),\n","            nn.BatchNorm1d(out_ch),\n","            nn.ReLU()\n","        )\n","        self.conv1_1 = nn.Sequential(\n","            nn.Conv1d(out_ch, out_ch, kernel_size=1, stride=1, padding=0, bias=True))\n","    def forward(self, x):\n","        x1 = self.conv(x)\n","        x1 = self.conv(x1)\n","        out=self.conv1_1(x1+x)\n","        return out\n","\n","\n","class R_1Dcnn_RCNN_block(nn.Module):\n","    def __init__(self, in_ch, out_ch, t=2):\n","        super(R_1Dcnn_RCNN_block, self).__init__()\n","\n","        self.RCNN1 = nn.Sequential(\n","            Recurrent_block(out_ch, t=t))\n","\n","        self.RCNN2 = nn.Sequential(\n","            conv1d_inception_block(out_ch, out_ch))\n","\n","        self.RCNN3 = nn.Sequential(\n","            Residual_block(out_ch, out_ch))\n","\n","        self.Conv = nn.Conv1d(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n","        self.Conv1_1 = nn.Sequential(\n","            nn.Conv1d(out_ch, out_ch, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm1d(out_ch),\n","            nn.ReLU())\n","\n","    def forward(self, x):\n","        x = self.Conv(x)\n","        x1 = self.RCNN3(x)\n","        x2 = self.RCNN2(x)\n","        x3 = self.RCNN1(x)\n","        out = self.Conv1_1(x3 + x2 + x1)\n","        return out\n","\n","\n","\n","\n","class R_inception_RCNN_block(nn.Module):\n","    \"\"\"\n","    Recurrent Residual Convolutional Neural Network Block\n","    \"\"\"\n","    def __init__(self, in_ch, out_ch, t=2):\n","        super(R_inception_RCNN_block, self).__init__()\n","\n","        self.RCNN1 = nn.Sequential(\n","            Recurrent_block(out_ch, t=t))\n","\n","        self.RCNN2 = nn.Sequential(\n","            conv1d_inception_block(out_ch, out_ch))\n","\n","        self.RCNN3 =nn.Sequential(\n","            Residual_block(out_ch, out_ch))\n","\n","\n","        self.Conv = nn.Conv1d(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n","        self.Conv1_1 = nn.Sequential(\n","            nn.Conv1d(out_ch, out_ch, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm1d(out_ch),\n","            nn.ReLU())\n","    def forward(self, x):\n","        x=self.Conv (x)\n","        x1 = self.RCNN3(x)\n","        x2 = self.RCNN2(x)\n","        x3 = self.RCNN1(x)\n","        out = self.Conv1_1(x3+x2+x1)\n","        return out\n","class Attention_block_self(nn.Module):\n","    \"\"\"\n","    Attention Block\n","    \"\"\"\n","\n","    def __init__(self, F_l, F_int):\n","        super(Attention_block_self, self).__init__()\n","\n","        self.W_g = nn.Sequential(\n","            nn.Conv1d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm1d(F_int)\n","        )\n","\n","        self.psi = nn.Sequential(\n","            nn.Conv1d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm1d(1),\n","            nn.Sigmoid()\n","        )\n","\n","        self.Tanh = nn.Tanh()\n","\n","    def forward(self,  x):\n","        x1 = self.W_g(x)\n","        psi = self.Tanh(x1)\n","        psi = self.psi(psi)\n","        out = x * psi\n","        return out\n","\n","\n","class model_1d(nn.Module):\n","    \"\"\"\n","    R2U-Unet implementation\n","    Paper: https://arxiv.org/abs/1802.06955\n","    \"\"\"\n","    def __init__(self, img_ch=1, output_ch=1, t=1):\n","        super(model_1d, self).__init__()\n","\n","        n1 =6\n","        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n","        self.Maxpool0 = nn.MaxPool1d(kernel_size=2, stride=2)\n","        self.Maxpool1 = nn.MaxPool1d(kernel_size=8, stride=8)\n","        self.Maxpool2 = nn.MaxPool1d(kernel_size=4, stride=4)\n","\n","        self.RRCNN1 = R_inception_RCNN_block(img_ch, filters[3], t=t)\n","\n","        self.RRCNN2 = R_inception_RCNN_block(filters[3], filters[2], t=t)\n","\n","        #self.RRCNN3 = R_inception_RCNN_block(filters[2], filters[2], t=t)\n","        self.RRCNN3 = R_inception_RCNN_block(filters[2], filters[2], t=t)\n","        #self.RRCNN4 = RRCNN_block(filters[3], filters[1], t=t)\n","        # self.Attention_block=Attention_block_self( F_l=filters[2], F_int=2)\n","        self.Softmax = nn.LogSoftmax()\n","        self.fc1 = nn.Linear(168,2)\n","        #self.fc2 = nn.Linear(3 ,3)\n","        #self.att= LinearSelfAttn()\n","        #self.fc2 = nn.Linear(24,1)\n","    def forward(self, x):\n","        x=self.Maxpool0(x)\n","        e1 = self.RRCNN1(x)\n","        #e1 = self.drop_layer(e1)\n","\n","        e2 = self.Maxpool2(e1)\n","\n","        e2 = self.RRCNN2(e2)\n","        #e2 = self.drop_layer(e2)\n","\n","        e3 = self.Maxpool2(e2)\n","        e3 = self.RRCNN3(e3)\n","        e3 = self.Maxpool2(e3)\n","        '''\n","        #e3 = self.drop_layer(e3)\n","\n","        e4 = self.Maxpool2(e3)\n","        e4 = self.RRCNN3(e4)\n","\n","        e4= self.Maxpool2(e3)\n","        '''\n","        e4=self.Maxpool2(e3)\n","        #e4 = self.Attention_block(x=e4)\n","        # print(e4.size(1))\n","        # print(e4.size(2))\n","        e7= e4.view(e4.size(0), e4.size(1)*e4.size(2))\n","        #e7=e4.view(-1, e4.view(-1).size(0))\n","        out = self.fc1(e7)\n","        #out = self.fc2(out)\n","        out =self.Softmax(out)\n","        #out= out.view(-1,24)\n","        #out=self.fc2(out)\n","        return out\n","\n","\n","def test(model,testloader):\n","     model.eval()\n","     test_loss = 0.0\n","     test_correct=0.0\n","     labelpredict=[]\n","     label=[]\n","     data=[]\n","     for inputs1, labels1 in testloader:\n","        inputs1, labels1 = Variable(inputs1.cuda()), Variable(labels1.cuda())\n","        # print(torch.cuda.is_initialized())\n","        output = model(inputs1)\n","\n","        data.append(inputs1.cpu().numpy())\n","        label.append(torch.argmax(labels1.cpu(),1).numpy())\n","        labelpredict.append(torch.argmax(output.cpu(),1))\n","     test_correct/=(len(testloader.dataset))\n","     return labelpredict ,data\n","\n","\n","# data=sio.loadmat('ecgpart_04.mat')\n","with open('testing_dataset.pkl', 'rb') as f:\n","     data = pickle.load(f)\n","# data = main_dataset.iloc[test_index].values\n","ecga=data[:, :4000]\n","print(len(ecga))\n","#useless-------------------------------------------------------------\n","# label1=np.zeros((len(ecga)-500,1))\n","# label2=np.zeros((250,1))+1\n","# label3=np.zeros((250,1))+2\n","# labelt=np.vstack((label1,label2,label3))\n","#---------------------------------------------------------------------\n","# label=sio.loadmat('CSPC2020label_04_0_1_by_manurally.mat')['sqi']\n","labelt = data[:, -1]\n","labelt = labelt.reshape(198, 1)\n","# labelt = label['sqi'][0:500]\n","print(labelt.shape)\n","for FF1 in range(len(ecga)):\n","    ecga[FF1,:]=butthigh(zscore(ecga[FF1,:]),400)\n","ecgt=torch.FloatTensor(ecga)\n","ecgt=ecgt.unsqueeze(1)\n","\n","labelt=to_categorical(labelt)\n","labelt=torch.FloatTensor(labelt)\n","print(labelt.size)\n","deal_test_dataset = TensorDataset(ecgt,labelt)\n","\n","testloader=DataLoader(dataset=deal_test_dataset,batch_size=32,shuffle=False,num_workers=0)\n","modelname='conmodel_4_epoch199.pkl'\n","model=torch.load(modelname)\n","# print(model)\n","model.eval()\n","labelpredict,testdata11=test(model,testloader)\n","j1=[]\n","for j in labelpredict:\n","    j2=j.numpy()\n","    j1.extend(j2)\n","# matname_result='2020label_04_result_trained_on_computer.mat'\n","# sio.savemat(matname_result,{\"predict\":j1})\n","filename = '2011_label_predict_by_model_trainied_on_patient_02.pkl'\n","import pickle\n","with open(filename, 'wb') as f:\n","  pickle.dump(j1, f)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kLfQQosR9IxH","executionInfo":{"status":"ok","timestamp":1697064663109,"user_tz":-360,"elapsed":876,"user":{"displayName":"Basob Paul Brinta","userId":"06538266531258667071"}},"outputId":"d6ef902a-f984-482a-e02d-1125d59095b4"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["198\n","(198, 1)\n","<built-in method size of Tensor object at 0x785127850b80>\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-19-34a3adbe3c7e>:305: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  out =self.Softmax(out)\n"]}]},{"cell_type":"code","source":["with open(\"testing_label_2020_patient_02.pkl\", 'wb') as f:\n","  pickle.dump(data[:, -1], f)"],"metadata":{"id":"XtAOsO4RpuEX","executionInfo":{"status":"ok","timestamp":1697063843894,"user_tz":-360,"elapsed":1075,"user":{"displayName":"Basob Paul Brinta","userId":"06538266531258667071"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["with open('testing_label_2020_patient_02.pkl', 'rb') as f:\n","    actual = pickle.load(f)"],"metadata":{"id":"junKEF0FsiZS","executionInfo":{"status":"ok","timestamp":1697063847340,"user_tz":-360,"elapsed":7,"user":{"displayName":"Basob Paul Brinta","userId":"06538266531258667071"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["actual.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T8egCYWfspl-","executionInfo":{"status":"ok","timestamp":1697063855749,"user_tz":-360,"elapsed":1066,"user":{"displayName":"Basob Paul Brinta","userId":"06538266531258667071"}},"outputId":"aec88adb-a448-4d80-d36b-c9ea373357bf"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1644,)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["with open('2020_label_predict_patient_02.pkl', 'rb') as f:\n","    predicted = pickle.load(f)"],"metadata":{"id":"rlzJFhJYsuGa","executionInfo":{"status":"ok","timestamp":1697063892831,"user_tz":-360,"elapsed":11,"user":{"displayName":"Basob Paul Brinta","userId":"06538266531258667071"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["np.array(predicted).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lzfN4Si5syAV","executionInfo":{"status":"ok","timestamp":1697063897864,"user_tz":-360,"elapsed":6,"user":{"displayName":"Basob Paul Brinta","userId":"06538266531258667071"}},"outputId":"675eb791-51b9-477e-f3e7-458f658848d5"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1644,)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["train_index.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2G-PL4l22CEC","executionInfo":{"status":"ok","timestamp":1697063134053,"user_tz":-360,"elapsed":1142,"user":{"displayName":"Basob Paul Brinta","userId":"06538266531258667071"}},"outputId":"df3a8191-ff19-4d48-8261-5187dfdecafa"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6574,)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["test_index.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3G1AR3TQ2Hqg","executionInfo":{"status":"ok","timestamp":1697063151660,"user_tz":-360,"elapsed":8,"user":{"displayName":"Basob Paul Brinta","userId":"06538266531258667071"}},"outputId":"4767d30f-e638-4183-cc6b-662a6d1061ed"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1644,)"]},"metadata":{},"execution_count":11}]}]}